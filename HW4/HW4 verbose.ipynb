{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning (CSCI-UA.473)\n",
    "\n",
    "## Homework 4: Long-Short Term Memory Networks and Ensemble of Neural Network Models\n",
    "### Due: November 18th, 2021 at 11:59PM\n",
    "\n",
    "\n",
    "### Name: Jerry Jia\n",
    "### Email: tj1043"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This homework has two parts. In the first part you will implement the long-short term memory networks and train them on a specific sequence modeling task. In the second part you will build an ensemble of neural networks for a specific problem and conduct some error analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question P1: Long-Short Term Memory Networks (35 Points Total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem you will implement a long-short term memory (LSTM) models from scratch and train them for the language modeling task. You will also do some error analysis and ablation studies on the learnt model. \n",
    "\n",
    "**At no point are you allowed to use PyTorch's nn.LSTM function in the code. Remember the goal of this assignment is to build the LSTM from scratch.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define some helper functions to handle the text data (do nothing here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "class Dictionary(object):\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.idx = 0\n",
    "    \n",
    "    def add_word(self, word):\n",
    "        if not word in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n",
    "\n",
    "\n",
    "class Corpus(object):\n",
    "    def __init__(self):\n",
    "        self.dictionary = Dictionary()\n",
    "\n",
    "    def get_data(self, path, batch_size=20):\n",
    "        # Add words to the dictionary\n",
    "        with open(path, 'r') as f:\n",
    "            tokens = 0\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                tokens += len(words)\n",
    "                for word in words: \n",
    "                    self.dictionary.add_word(word)  \n",
    "        \n",
    "        # Tokenize the file content\n",
    "        ids = torch.LongTensor(tokens)\n",
    "        token = 0\n",
    "        with open(path, 'r') as f:\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                for word in words:\n",
    "                    ids[token] = self.dictionary.word2idx[word]\n",
    "                    token += 1\n",
    "        num_batches = ids.size(0) // batch_size\n",
    "        ids = ids[:num_batches*batch_size]\n",
    "        return ids.view(batch_size, -1)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define some hyper-parameters of the model (do nothing here.. yet)\n",
    "We will now define some hyper-parameters to be used with the model. Later on in the assignment you will be experimenting with some of these. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "\n",
    "# Hyper-parameters\n",
    "embed_size = 32         # size of the input feature vector representing each word\n",
    "hidden_size = 32        # number of hidden units in the LSTM cell\n",
    "num_epochs = 1          # number of epochs for which you will train your model    \n",
    "num_samples = 200       # number of words to be sampled\n",
    "batch_size = 20         # the size of your mini-batch\n",
    "seq_length = 30         # the size of the BPTT window\n",
    "learning_rate = 0.002   # learning rate of the model\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce GTX 1060'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.current_device()  #returns you the ID of your current device\n",
    "torch.cuda.get_device_name(0) #returns you the name of the device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the sequential data (do nothing here)\n",
    "\n",
    "We will use the Penn Tree Bank dataset for the purpose of this exercise. You need to download by running the command `wget https://data.deepai.org/ptbdataset.zip`, unzip it, and store it in the directory `./data/ptb`. We will only use the files `ptb.train.txt` and `ptb.test.txt`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1549\n",
      "137\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset \n",
    "corpus = Corpus()\n",
    "ids = corpus.get_data('./data/ptb/ptb.train.txt', batch_size)\n",
    "vocab_size = len(corpus.dictionary)\n",
    "num_batches = ids.size(1) // seq_length\n",
    "\n",
    "test_corpus = Corpus()\n",
    "test_ids = test_corpus.get_data('./data/ptb/ptb.test.txt', batch_size)\n",
    "test_vocab_size =  len(test_corpus.dictionary)\n",
    "test_num_batches = test_ids.size(1) // seq_length\n",
    "\n",
    "print(num_batches)\n",
    "print(test_num_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the Long-Short Term Memory (LSTM) unit (20 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will be implementing a single unit of the LSTM and also write its forward propagation algorithm. The single LSTM unit performs the following operations: \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "i_t & = \\sigma (W_{ii} x_t + b_{ii} + W_{hi} h_{t-1} + b_{hi}) \\\\\n",
    "f_t & = \\sigma (W_{if} x_t + b_{if} + W_{hf} h_{t-1} + b_{hf}) \\\\\n",
    "g_t & = \\sigma (W_{ig} x_t + b_{ig} + W_{hg} h_{t-1} + b_{hg}) \\\\\n",
    "o_t & = \\sigma (W_{io} x_t + b_{io} + W_{ho} h_{t-1} + b_{ho}) \\\\\n",
    "c_t & = f_t \\odot c_{t-1} + i_t \\odot g_t \\\\\n",
    "h_t & = o_t \\odot \\tanh(c_t)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $h_t$ is the hidden state at time $t$, $c_t$ is called the memory cell state at time $t$, $x_t$ is the input at time $t$, $h_{t-1}$ is the hidden state of the layer at time $t-1$ or the initial hidden state at time $0$, and $i_t$, $f_t$, $g_t$, $o_t$, are the input, forget, cell, and output gates, respectively. $\\sigma$ is the sigmoid function, and $\\odot$ is the Hadamard (element-wise) product of two vectors. Finally $\\{W_{ii}, b_{ii}, W_{hi}, b_{hi}\\}$, $\\{W_{if}, b_{if}, W_{hf}, b_{hf}\\}$, $\\{W_{ig}, b_{ig}, W_{hg}, b_{hg}\\}$, and $\\{W_{io}, b_{io}, W_{ho}, b_{ho}\\}$ are the learnable weights and biases for computing the input, forget, cell, and output gates respectively.\n",
    "\n",
    "At each time step the LSTM takes as input the previous hidden state, the previous memory cell state and the embedding (features) associated with the current word and generates the new hidden states and the prediction of the next word. \n",
    "\n",
    "**Hint: Note that the recurrsion is around two variables, namely, $h_t$ and $c_t$. Hence you will need to keep track of two previous hidden states.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def s(x, n=''):\n",
    "    # return\n",
    "    o = list(x.size())\n",
    "    print(n, o)\n",
    "    # print(o[0] if len(o) == 1 else ', '.join(o))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# myLSTM based language model\n",
    "sig = torch.sigmoid\n",
    "tanh = torch.tanh\n",
    "\n",
    "class myLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
    "        \"\"\"\n",
    "        Input: \n",
    "            vocab_size: the number of unique words in your dataset \n",
    "            embed_size: the size of the feature vector assiciated with each word\n",
    "            hidden_size: the size of the hidden state features\n",
    "        \"\"\"\n",
    "        super(myLSTM, self).__init__()\n",
    "        self.embed = nn.Embedding(\n",
    "            vocab_size, embed_size\n",
    "        )  # features associated with the words in your vocabulary\n",
    "\n",
    "        # rest of your LSTM model code goes here\n",
    "\n",
    "        self.fi = nn.Linear(embed_size, hidden_size)\n",
    "        self.ii = nn.Linear(embed_size, hidden_size)\n",
    "        self.gi = nn.Linear(embed_size, hidden_size)\n",
    "        self.oi = nn.Linear(embed_size, hidden_size)\n",
    "\n",
    "        self.fh = nn.Linear(hidden_size, hidden_size)\n",
    "        self.ih = nn.Linear(hidden_size, hidden_size)\n",
    "        self.gh = nn.Linear(hidden_size, hidden_size)\n",
    "        self.oh = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, states):\n",
    "        \"\"\"\n",
    "        Perform forward propagation over a single unit composed of an LSTM unit followed by a linear layer to \n",
    "        generate the input activations for the softmax function. \n",
    "    \n",
    "        Input:\n",
    "            x: the current input (indices into the vocabulary)\n",
    "            h: the previous hidden states\n",
    "\n",
    "        Output:\n",
    "            out: the output of the unit (which are the input activations of the softmax layer to predict the next word)\n",
    "            (h, c): the current hidden states composed of the hidden state and the memory cell state\n",
    "        \"\"\"\n",
    "        h_prev, c_prev = states\n",
    "        # s(x, 'x')\n",
    "        embeds = self.embed(x)\n",
    "        # s(embeds, 'embeds')\n",
    "        h = []\n",
    "\n",
    "        for t in range(x.shape[1]):\n",
    "            x_t = embeds[:, t, :]\n",
    "\n",
    "            f_t = sig(self.fi(x_t) + self.fh(h_prev))\n",
    "            i_t = sig(self.ii(x_t) + self.ih(h_prev))\n",
    "            g_t = tanh(self.gi(x_t) + self.gh(h_prev))\n",
    "            o_t = sig(self.oi(x_t) + self.oh(h_prev))\n",
    "\n",
    "            c_t = f_t * c_prev + i_t * g_t\n",
    "            h_t = o_t * tanh(c_t)\n",
    "            # s(h_t, 'h_t')\n",
    "            h.append(h_t)\n",
    "\n",
    "        h = torch.stack(h, dim=2)\n",
    "        if h.dim() == 4:\n",
    "            h = h[-1, :,:,:]\n",
    "        elif h.dim() == 3:\n",
    "            h = h[-1,:,-1]\n",
    "        out = sig(self.linear(h))\n",
    "\n",
    "        return out, (h_t, c_t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop for the model (7.5 Points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7536/2822045629.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmyLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Loss and optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ml/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mcuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    678\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m         \"\"\"\n\u001b[0;32m--> 680\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mxpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ml/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ml/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    591\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    594\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ml/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    678\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m         \"\"\"\n\u001b[0;32m--> 680\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mxpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Truncated backpropagation\n",
    "def detach(states):\n",
    "    return [state.detach() for state in states]\n",
    "\n",
    "\n",
    "model = myLSTM(vocab_size, embed_size, hidden_size).cuda()\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # the loss function\n",
    "\n",
    "# we will use the Adam optimizer for faster and easier convergence\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    # Set initial hidden and cell states\n",
    "    states = (torch.zeros(1, batch_size, hidden_size).to(device),\n",
    "              torch.zeros(1, batch_size, hidden_size).to(device))\n",
    "\n",
    "    for i in range(0, ids.size(1) - seq_length, seq_length):\n",
    "        # Get mini-batch inputs and targets\n",
    "        inputs = ids[:, i:i + seq_length].to(device)\n",
    "        targets = ids[:, (i + 1):(i + 1) + seq_length].to(device)\n",
    "\n",
    "        states = detach(states)\n",
    "\n",
    "        outputs, states = model(inputs, states)\n",
    "\n",
    "        outputs = outputs[:, -1, :]\n",
    "        # s(outputs, 'outputs')\n",
    "        # print(outputs.argmax(dim=1))\n",
    "        targets = targets[:, -1]\n",
    "        # for a, b in zip(outputs.argmax(dim=1), targets):\n",
    "        #     t = corpus.dictionary.idx2word[b.item()]\n",
    "        #     p = corpus.dictionary.idx2word[a.item()]\n",
    "        #     print(p, t)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Clear the gradient buffers of the optimized parameters.\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # we clip the gradients to ensure that they remain bounded. This is a hack!\n",
    "        clip_grad_norm_(model.parameters(), 0.5)\n",
    "\n",
    "        # Optimize\n",
    "        optimizer.step()\n",
    "        step = (i + 1) // seq_length\n",
    "        if step % 100 == 0:\n",
    "            print(\n",
    "                'Epoch [{}/{}], Step[{}/{}], Loss: {:.4f}, Perplexity: {:5.2f}'\n",
    "                .format(epoch + 1, num_epochs, step, num_batches, loss.item(),\n",
    "                        np.exp(loss.item())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './p1.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torchLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class torchLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
    "        \"\"\"\n",
    "        Input: \n",
    "            vocab_size: the number of unique words in your dataset \n",
    "            embed_size: the size of the feature vector assiciated with each word\n",
    "            hidden_size: the size of the hidden state features\n",
    "        \"\"\"\n",
    "        super(torchLSTM, self).__init__()\n",
    "        self.embed = nn.Embedding(\n",
    "            vocab_size, embed_size\n",
    "        )  # features associated with the words in your vocabulary\n",
    "\n",
    "        # rest of your LSTM model code goes here\n",
    "\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
    "\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, states):\n",
    "        embed = self.embed(x)\n",
    "        # s(embed, 'embed')\n",
    "        output, states = self.lstm(embed, states)\n",
    "        # s(output, 'lstm output')\n",
    "        # s(states[0], 'states.h')\n",
    "        decoded = self.linear(output)\n",
    "        return decoded, states\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.05\n",
    "        self.embed.weight.data.uniform_(-initrange, initrange)\n",
    "        self.linear.bias.data.fill_(0)\n",
    "        self.linear.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def init_hidden(self, bsz):\n",
    "        # LSTM h and c\n",
    "        weight = next(self.parameters()).data\n",
    "        return weight.new_zeros(1, bsz, hidden_size), weight.new_zeros(1, bsz, hidden_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop for the model (7.5 Points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Truncated backpropagation\n",
    "def detach(states):\n",
    "    return [state.detach() for state in states]\n",
    "\n",
    "\n",
    "model2 = torchLSTM(vocab_size, embed_size, hidden_size).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # the loss function\n",
    "\n",
    "# we will use the Adam optimizer for faster and easier convergence\n",
    "optimizer2 = torch.optim.Adam(model2.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model2\n",
    "model2.train()\n",
    "for epoch in range(num_epochs):\n",
    "    states = model2.init_hidden(batch_size)\n",
    "    for i in range(0, ids.size(1) - seq_length, seq_length):\n",
    "        # Get mini-batch inputs and targets\n",
    "        inputs = ids[:, i:i + seq_length].to(device)\n",
    "        targets = ids[:, (i + 1):(i + 1) + seq_length].to(device)\n",
    "\n",
    "        states = detach(states)\n",
    "        outputs, states = model2(inputs, states)\n",
    "        s(outputs, 'outputs')\n",
    "        # s(targets, 'targets')\n",
    "        outputs = outputs[:, -1, :]\n",
    "        targets = targets[:, -1]\n",
    "\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Clear the gradient buffers of the optimized parameters.\n",
    "        optimizer2.zero_grad()\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # we clip the gradients to ensure that they remain bounded. This is a hack!\n",
    "        clip_grad_norm_(model2.parameters(), 0.5)\n",
    "\n",
    "        # Optimize\n",
    "        optimizer2.step()\n",
    "\n",
    "        step = (i + 1) // seq_length\n",
    "        if step % 100 == 0:\n",
    "            print(\n",
    "                'Epoch [{}/{}], Step[{}/{}], Loss: {:.4f}, Perplexity: {:5.2f}'\n",
    "                .format(epoch + 1, num_epochs, step, num_batches, loss.item(),\n",
    "                        np.exp(loss.item())))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing loop for the model (7.5 Points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7536/1231513283.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m## code for forward pass goes here ##\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# Test the model\n",
    "states = (torch.zeros(1, batch_size, hidden_size).to(device),\n",
    "              torch.zeros(1, batch_size, hidden_size).to(device))\n",
    "test_loss = 0.\n",
    "with torch.no_grad():\n",
    "    for i in range(0, test_ids.size(1) - seq_length, seq_length):\n",
    "        # Get mini-batch inputs and targets\n",
    "        inputs = test_ids[:, i:i+seq_length].to(device)\n",
    "        targets = test_ids[:, (i+1):(i+1)+seq_length].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        states = detach(states)\n",
    "\n",
    "        ## code for forward pass goes here ##\n",
    "        outputs, states = model.forward(inputs, states)\n",
    "\n",
    "        outputs = outputs[:, -1, :]\n",
    "        targets = targets[:, -1]\n",
    "        # s(outputs, 'outputs')\n",
    "        # s(outputs.argmax(dim=1), 'outputs')\n",
    "        print(outputs.argmax(dim=1))\n",
    "        s(targets, 'targets')\n",
    "        for a, b in zip(outputs.argmax(dim=1), targets):\n",
    "            t = corpus.dictionary.idx2word[b.item()]\n",
    "            p = corpus.dictionary.idx2word[a.item()]\n",
    "            print(p, t)\n",
    "\n",
    "        test_loss = criterion(outputs, targets)\n",
    "\n",
    "test_loss = test_loss / test_num_batches\n",
    "print('test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
    "    test_loss, math.exp(test_loss)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating text using the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = myLSTM(vocab_size, embed_size, hidden_size).cuda()\n",
    "\n",
    "model.load_state_dict(torch.load('./p1.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled [100/200] words and save to sample.txt\n",
      "Sampled [200/200] words and save to sample.txt\n"
     ]
    }
   ],
   "source": [
    "model = myLSTM(vocab_size, embed_size, hidden_size).cuda()\n",
    "\n",
    "model.load_state_dict(torch.load('./p1.pt'))\n",
    "# Generate texts using trained model\n",
    "with torch.no_grad():\n",
    "    with open('sample.txt', 'w') as f:\n",
    "        # Set intial hidden ane cell states\n",
    "        state = (torch.zeros(1, hidden_size).to(device),\n",
    "                 torch.zeros(1, hidden_size).to(device))\n",
    "\n",
    "        # Select one word id randomly\n",
    "        prob = torch.ones(vocab_size)\n",
    "        input = torch.multinomial(prob, num_samples=1).unsqueeze(1).to(device)\n",
    "\n",
    "        for i in range(num_samples):\n",
    "            # Forward propagate RNN\n",
    "            output, state = model(input, state)\n",
    "\n",
    "            # Sample a word id\n",
    "            prob = output.exp()\n",
    "            word_id = torch.multinomial(prob, num_samples=1).item()\n",
    "\n",
    "            # Fill input with sampled word id for the next time step\n",
    "            input.fill_(word_id)\n",
    "\n",
    "            # File write\n",
    "            word = corpus.dictionary.idx2word[word_id]\n",
    "            word = '\\n' if word == '<eos>' else word + ' '\n",
    "            f.write(word)\n",
    "\n",
    "            if (i+1) % 100 == 0:\n",
    "                print('Sampled [{}/{}] words and save to {}'.format(i+1, num_samples, 'sample.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question P2: Ensemble of Neural Networks (35 Points Total)\n",
    "In this part of the assignment, you will train an ensemble of neural networks for classifying hand written digits in the MNIST dataset. You will take the code provided in the following cell and extend it to build an ensemble of single hidden layer MLP as per the specifications provided in the questions below. \n",
    "\n",
    "For the data, you need to download the MNIST dataset from ``http://yann.lecun.com/exdb/mnist/`` and place the files in the directory ``./data/MNIST``. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# load the MNIST dataset\n",
    "input_size  = 28*28   # images are 28x28 pixels\n",
    "output_size = 10      # there are 10 classes\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./data/MNIST', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=64, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./data/MNIST', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=1000, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the multi-layer perceptron model\n",
    "class FC2Layer(nn.Module):\n",
    "    def __init__(self, input_size, n_hidden, output_size):\n",
    "        super(FC2Layer, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_size, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden, output_size),\n",
    "            nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.input_size)\n",
    "        return self.network(x)\n",
    "\n",
    "# define the training loop\n",
    "def train(epoch, model):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass through the model\n",
    "        output = model(data)\n",
    "        # forward pass through the cross-entropy loss function\n",
    "        loss = F.nll_loss(output, target)\n",
    "        # backward pass through the cross-entropy loss function and the model\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "\n",
    "# define the testing loop\n",
    "def test(model):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        # data, target = data.to(device), target.to(device)\n",
    "\n",
    "        output = model(data)\n",
    "        test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        print(torch.flatten(pred))\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    accuracy_list = []\n",
    "    accuracy_list.append(accuracy)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        accuracy))\n",
    "\n",
    "\n",
    "# function to count number of parameters\n",
    "def get_n_params(model):\n",
    "    np = 0\n",
    "    for p in list(model.parameters()):\n",
    "        np += p.nelement()\n",
    "    return np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model and execute the training and testing\n",
    "# initialize some hyper-paramters\n",
    "n_hidden = 4  # number of hidden units\n",
    "learning_rate = 0.01\n",
    "moment = 0.5\n",
    "nepochs = 1\n",
    "\n",
    "# build the actual model\n",
    "model_fnn = FC2Layer(input_size, n_hidden, output_size)\n",
    "# model_fnn.to(device)\n",
    "# initialize the optimizer\n",
    "optimizer = optim.SGD(model_fnn.parameters(),\n",
    "                      lr=learning_rate,\n",
    "                      momentum=moment)\n",
    "print('Number of parameters: {}'.format(get_n_params(model_fnn)))\n",
    "\n",
    "# train the model for one epoch\n",
    "for epoch in range(0, nepochs):\n",
    "    train(epoch, model_fnn)\n",
    "    test(model_fnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question P2.a: Ensemble of networks of same size but initialized with a different seed (20 Points)\n",
    "Extend the above code to create an ensemble of `k` single hidden layer MLP models, each initialized with a different random seed. Train each model inside the ensemble for exactly `1` epoch and report the performance of the individual models and the ensemble model on the test set. For the purpose of ensembling, you can take a majority vote of the model outputs. \n",
    "\n",
    "At a high level you need to perform the following tasks:\n",
    "1. Initialize `k` models with different random seed (for some value of k): (**5 points**)\n",
    "2. Extend the train() function to train individual models inside the ensemble: (**5 points**)\n",
    "3. Extend the test() function to estimate the accuracy of the individual models and the ensemble model: (**5 points**)\n",
    "4. Repeat the above process with different values of `k`. For this exercise use $k = \\{1, 2, 4, 8, 16, 32 \\}$, and plot a graph with `k` on the x-axis and ensemble model performance on the y-axis. (**5 points**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsembleMLP(nn.Module):\n",
    "    def __init__(self,\n",
    "                 k,\n",
    "                 input_size,\n",
    "                 n_hidden,\n",
    "                 output_size,\n",
    "                 learning_rate=learning_rate,\n",
    "                 moment=moment,\n",
    "                 verbose=True,\n",
    "                 manual_seed=None):\n",
    "        self.verbose = verbose\n",
    "        super(EnsembleMLP, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.k = k\n",
    "        self.models = nn.ModuleList()\n",
    "        self.optimizers = []\n",
    "        if manual_seed is not None:\n",
    "            torch.manual_seed(manual_seed)\n",
    "        for i in range(k):\n",
    "            if manual_seed is None: torch.manual_seed(i)\n",
    "            model = FC2Layer(input_size, n_hidden, output_size)\n",
    "            optimizer = optim.SGD(model.parameters(),\n",
    "                                  lr=learning_rate,\n",
    "                                  momentum=moment)\n",
    "            self.models.append(model)\n",
    "            self.optimizers.append(optimizer)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.input_size)\n",
    "        return [model.network(x) for model in self.models]\n",
    "\n",
    "    # define the training loop\n",
    "    def train(self, epoch, loader=train_loader):\n",
    "        for i, (model,\n",
    "                optimizer) in enumerate(zip(self.models, self.optimizers)):\n",
    "            model.train()\n",
    "\n",
    "            for batch_idx, (data, target) in enumerate(loader):\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                # forward pass through the model\n",
    "                output = model(data)\n",
    "                # forward pass through the cross-entropy loss function\n",
    "                loss = F.nll_loss(output, target)\n",
    "                # backward pass through the cross-entropy loss function and the model\n",
    "                loss.backward()\n",
    "\n",
    "                optimizer.step()\n",
    "                if batch_idx % 200 == 0 and self.verbose:\n",
    "                    print('Model {} Epoch {}: [{}/{} ({:.0f}%)]\\tTrain Loss: {:.6f}'.\n",
    "                          format(i, epoch, batch_idx * len(data),\n",
    "                                 len(loader.dataset),\n",
    "                                 100. * batch_idx / (len(loader) - 1),\n",
    "                                 loss.item()))\n",
    "                \n",
    "                if batch_idx == len(loader)-1:\n",
    "                    print('Model {} Epoch {} Final Train Loss: {:.6f}'.\n",
    "                        format(i, epoch, loss.item()))\n",
    "\n",
    "    def eval(self):\n",
    "        for model in self.models:\n",
    "            model.eval()\n",
    "\n",
    "    def test_one(self, model, loader=test_loader):\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        pred_list = []\n",
    "        for data, target in loader:\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(\n",
    "                output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.data.max(\n",
    "                1, keepdim=True)[1]  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.data.view_as(pred)).cpu().sum().item()\n",
    "            pred_list.append(pred)\n",
    "\n",
    "        test_loss /= len(loader.dataset)\n",
    "        accuracy = 100. * correct / len(loader.dataset)\n",
    "        print(\n",
    "            '\\nk={} Ensemble Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'\n",
    "            .format(self.k, test_loss, correct, len(loader.dataset),\n",
    "                    accuracy))\n",
    "        return test_loss, accuracy\n",
    "\n",
    "    def test(self):\n",
    "        if self.k == 1:\n",
    "            return self.test_one(self.models[0])\n",
    "        self.eval()\n",
    "        test_loss = torch.zeros(self.k)\n",
    "        correct = torch.zeros(self.k)\n",
    "        ensemble_test_loss = 0\n",
    "        ensemble_correct = 0\n",
    "        test_size = len(test_loader.dataset)\n",
    "        for data, target in test_loader:\n",
    "            preds = []\n",
    "            for m, model in enumerate(self.models):\n",
    "                output = model(data)\n",
    "                test_loss[m] += F.nll_loss(\n",
    "                    output, target,\n",
    "                    reduction='sum').item()  # sum up batch loss\n",
    "                pred = output.data.max(\n",
    "                    1, keepdim=True)[1]  # get the index of the max log-probability\n",
    "                preds.append(pred)\n",
    "\n",
    "                correct[m] += pred.eq(\n",
    "                    target.data.view_as(pred)).cpu().sum().item()\n",
    "\n",
    "            ensemble_pred = torch.mode(torch.stack(preds), 0).values\n",
    "            # print(ensemble_pred)\n",
    "            ensemble_correct += ensemble_pred.eq(target.data.view_as(ensemble_pred)).cpu().sum().item()\n",
    "\n",
    "        for m, model in enumerate(self.models):\n",
    "            test_loss[m] /= test_size\n",
    "            accuracy = 100. * correct[m] / test_size\n",
    "            print(\n",
    "                'Model {} Test Loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'\n",
    "                .format(m, test_loss[m].item(), correct[m].item(), test_size,\n",
    "                        accuracy.item()))\n",
    "\n",
    "        ensemble_test_loss = test_loss.mean()\n",
    "        ensemble_accuracy = 100. * ensemble_correct / test_size\n",
    "        print('\\nk={} Ensemble Average Loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.\n",
    "              format(self.k, ensemble_test_loss, ensemble_correct,\n",
    "                     test_size, ensemble_accuracy))\n",
    "        return ensemble_test_loss, ensemble_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize some hyper-paramters\n",
    "n_hidden = 4  # number of hidden units\n",
    "learning_rate = 0.01\n",
    "moment = 0.5\n",
    "nepochs = 1\n",
    "\n",
    "# build the actual model\n",
    "model_ensemble = EnsembleMLP(2, input_size, n_hidden, output_size)\n",
    "# model_ensemble.to(device)\n",
    "\n",
    "# train the model for one epoch\n",
    "for epoch in range(nepochs):\n",
    "    print(f'======== TRAIN EPOCH {epoch} ========')\n",
    "    model_ensemble.train(epoch)\n",
    "    torch.save(model_ensemble.state_dict(), './model_ensemble')\n",
    "\n",
    "    print(f'\\n======== TEST EPOCH {epoch} ========')\n",
    "    model_ensemble.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ensemble = EnsembleMLP(2, input_size, n_hidden, output_size)\n",
    "model_ensemble.load_state_dict(torch.load('./model_ensemble'))\n",
    "model_ensemble.test()\n",
    "# print(model_ensemble.state_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k_list = [1, 2, 4, 8, 16, 32]\n",
    "k_list = [1, 2, 4, 8, 16]\n",
    "k_loss = []\n",
    "k_acc = []\n",
    "for k in k_list:\n",
    "    print(f'======== k={k} ========')\n",
    "    model_ensemble = EnsembleMLP(k,\n",
    "                                 input_size,\n",
    "                                 n_hidden,\n",
    "                                 output_size,\n",
    "                                 verbose=False)\n",
    "    for epoch in range(nepochs):\n",
    "        model_ensemble.train(epoch)\n",
    "    loss, acc = model_ensemble.test()\n",
    "    k_loss.append(loss)\n",
    "    k_acc.append(acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph\n",
    "fig, (ax1, ax2)=plt.subplots(1,2,figsize=(14, 7))\n",
    "fig.suptitle('Ensembled MLP', fontsize=16)\n",
    "\n",
    "# plt.subplot(1, 2, 1)  # row 1, column 2, count 1\n",
    "ax1.plot(k_list, k_loss, '-o', label='Loss', color='orange')\n",
    "ax1.set_title('Loss')\n",
    "ax1.set_xlabel('k')\n",
    "ax1.set_ylabel('Loss')\n",
    "\n",
    "ax2.plot(k_list, k_acc, '-o', label='Accuracy')\n",
    "ax2.set_title('Accuracy')\n",
    "ax2.set_xlabel('k')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question P2.b: Bagged networks  (15 Points)\n",
    "Extend the above code to create a bag of `k` single hidden layer MLP models with each with `4` hidden units. Compare the performance of bags of different sizes `k`. In particular, compare the performance of `5` bages, with sizes $k = \\{2, 4, 8, 16, 32\\}$ respectively. To prevent additional sources of randomness, all the models within all the bags should be initialized to the same random seed. Train each model inside each bag for exactly `1` epoch. Report the final performance for each bag on the test set and compare it against each other. For the purpose of ensembling, you can take a majority vote of the model outputs. \n",
    "\n",
    "At a high level you need to perform the following tasks:\n",
    "1. Create `k=5` bags of the training set of sizes $k = \\{2, 4, 8, 16, 32\\}$. (**5 points**)\n",
    "2. Use the same extension to the train() function as in P2.a to train individual models in each bag: (**2.5 points**)\n",
    "3. Use the same extension to the test() function as in P2.a to estimate the accuracy for each bag: (**2.5 points**)\n",
    "4. Plot a graph with bag_size on the x-axis and bag performance on the y-axis. (**5 points**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.MNIST('./data/MNIST',\n",
    "                               train=True,\n",
    "                               download=True,\n",
    "                               transform=transforms.Compose([\n",
    "                                   transforms.ToTensor(),\n",
    "                                   transforms.Normalize((0.1307, ), (0.3081, ))\n",
    "                               ]))\n",
    "bootstrap_sampler = torch.utils.data.RandomSampler(train_dataset,\n",
    "                                                   replacement=True)\n",
    "                                                   \n",
    "bag_train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                               sampler=bootstrap_sampler,\n",
    "                                               batch_size=64,\n",
    "                                               drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize some hyper-paramters\n",
    "n_hidden = 4  # number of hidden units\n",
    "learning_rate = 0.01\n",
    "moment = 0.5\n",
    "nepochs = 1\n",
    "\n",
    "# build the actual model\n",
    "model_ensemble = EnsembleMLP(2, input_size, n_hidden, output_size, verbose=False)\n",
    "# model_ensemble.to(device)\n",
    "\n",
    "# train the model for one epoch\n",
    "for epoch in range(nepochs):\n",
    "    print(f'======== TRAIN EPOCH {epoch} ========')\n",
    "    model_ensemble.train(epoch, loader=bag_train_loader)\n",
    "    torch.save(model_ensemble.state_dict(), './model_ensemble')\n",
    "\n",
    "    print(f'\\n======== TEST EPOCH {epoch} ========')\n",
    "    model_ensemble.test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k_list = [2, 4, 8, 16, 32]\n",
    "bagged_k_list = [2, 4, 8, 16]\n",
    "bagged_k_loss = []\n",
    "bagged_k_acc = []\n",
    "for k in bagged_k_list:\n",
    "    print(f'======== k={k} ========')\n",
    "    model_ensemble = EnsembleMLP(k,\n",
    "                                 input_size,\n",
    "                                 n_hidden,\n",
    "                                 output_size,\n",
    "                                 manual_seed=42,\n",
    "                                 verbose=False)\n",
    "    for epoch in range(nepochs):\n",
    "        model_ensemble.train(epoch, loader=bag_train_loader)\n",
    "    loss, acc = model_ensemble.test()\n",
    "    bagged_k_loss.append(loss)\n",
    "    bagged_k_acc.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 7))\n",
    "fig.suptitle('Bootstrap Aggregated MLP', fontsize=16)\n",
    "\n",
    "# plt.subplot(1, 2, 1)  # row 1, column 2, count 1\n",
    "ax1.plot(bagged_k_list, bagged_k_loss, '-o', label='Loss', color='orange')\n",
    "ax1.set_title('Loss')\n",
    "ax1.set_xlabel('k')\n",
    "ax1.set_ylabel('Loss')\n",
    "\n",
    "ax2.plot(bagged_k_list, bagged_k_acc, '-o', label='Accuracy')\n",
    "ax2.set_title('Accuracy')\n",
    "ax2.set_xlabel('k')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "374px",
    "left": "1310px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
