{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning (CSCI-UA.473)\n",
    "\n",
    "## Homework 4: Long-Short Term Memory Networks and Ensemble of Neural Network Models\n",
    "### Due: November 18th, 2021 at 11:59PM\n",
    "\n",
    "\n",
    "### Name: Jerry Jia\n",
    "### Email: tj1043"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This homework has two parts. In the first part you will implement the long-short term memory networks and train them on a specific sequence modeling task. In the second part you will build an ensemble of neural networks for a specific problem and conduct some error analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question P1: Long-Short Term Memory Networks (35 Points Total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem you will implement a long-short term memory (LSTM) models from scratch and train them for the language modeling task. You will also do some error analysis and ablation studies on the learnt model. \n",
    "\n",
    "**At no point are you allowed to use PyTorch's nn.LSTM function in the code. Remember the goal of this assignment is to build the LSTM from scratch.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define some helper functions to handle the text data (do nothing here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "class Dictionary(object):\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.idx = 0\n",
    "    \n",
    "    def add_word(self, word):\n",
    "        if not word in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n",
    "\n",
    "\n",
    "class Corpus(object):\n",
    "    def __init__(self):\n",
    "        self.dictionary = Dictionary()\n",
    "\n",
    "    def get_data(self, path, batch_size=20):\n",
    "        # Add words to the dictionary\n",
    "        with open(path, 'r') as f:\n",
    "            tokens = 0\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                tokens += len(words)\n",
    "                for word in words: \n",
    "                    self.dictionary.add_word(word)  \n",
    "        \n",
    "        # Tokenize the file content\n",
    "        ids = torch.LongTensor(tokens)\n",
    "        token = 0\n",
    "        with open(path, 'r') as f:\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                for word in words:\n",
    "                    ids[token] = self.dictionary.word2idx[word]\n",
    "                    token += 1\n",
    "        num_batches = ids.size(0) // batch_size\n",
    "        ids = ids[:num_batches*batch_size]\n",
    "        return ids.view(batch_size, -1)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define some hyper-parameters of the model (do nothing here.. yet)\n",
    "We will now define some hyper-parameters to be used with the model. Later on in the assignment you will be experimenting with some of these. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyper-parameters\n",
    "embed_size = 32         # size of the input feature vector representing each word\n",
    "hidden_size = 32        # number of hidden units in the LSTM cell\n",
    "num_epochs = 1          # number of epochs for which you will train your model    \n",
    "num_samples = 200       # number of words to be sampled\n",
    "batch_size = 20         # the size of your mini-batch\n",
    "seq_length = 30         # the size of the BPTT window\n",
    "learning_rate = 0.002   # learning rate of the model\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the sequential data (do nothing here)\n",
    "\n",
    "We will use the Penn Tree Bank dataset for the purpose of this exercise. You need to download by running the command `wget https://data.deepai.org/ptbdataset.zip`, unzip it, and store it in the directory `./data/ptb`. We will only use the files `ptb.train.txt` and `ptb.test.txt`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1549\n",
      "137\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset \n",
    "corpus = Corpus()\n",
    "ids = corpus.get_data('./data/ptb/ptb.train.txt', batch_size)\n",
    "vocab_size = len(corpus.dictionary)\n",
    "num_batches = ids.size(1) // seq_length\n",
    "\n",
    "test_corpus = Corpus()\n",
    "test_ids = test_corpus.get_data('./data/ptb/ptb.test.txt', batch_size)\n",
    "test_vocab_size =  len(test_corpus.dictionary)\n",
    "test_num_batches = test_ids.size(1) // seq_length\n",
    "\n",
    "print(num_batches)\n",
    "print(test_num_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the Long-Short Term Memory (LSTM) unit (20 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will be implementing a single unit of the LSTM and also write its forward propagation algorithm. The single LSTM unit performs the following operations: \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "i_t & = \\sigma (W_{ii} x_t + b_{ii} + W_{hi} h_{t-1} + b_{hi}) \\\\\n",
    "f_t & = \\sigma (W_{if} x_t + b_{if} + W_{hf} h_{t-1} + b_{hf}) \\\\\n",
    "g_t & = \\sigma (W_{ig} x_t + b_{ig} + W_{hg} h_{t-1} + b_{hg}) \\\\\n",
    "o_t & = \\sigma (W_{io} x_t + b_{io} + W_{ho} h_{t-1} + b_{ho}) \\\\\n",
    "c_t & = f_t \\odot c_{t-1} + i_t \\odot g_t \\\\\n",
    "h_t & = o_t \\odot \\tanh(c_t)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $h_t$ is the hidden state at time $t$, $c_t$ is called the memory cell state at time $t$, $x_t$ is the input at time $t$, $h_{t-1}$ is the hidden state of the layer at time $t-1$ or the initial hidden state at time $0$, and $i_t$, $f_t$, $g_t$, $o_t$, are the input, forget, cell, and output gates, respectively. $\\sigma$ is the sigmoid function, and $\\odot$ is the Hadamard (element-wise) product of two vectors. Finally $\\{W_{ii}, b_{ii}, W_{hi}, b_{hi}\\}$, $\\{W_{if}, b_{if}, W_{hf}, b_{hf}\\}$, $\\{W_{ig}, b_{ig}, W_{hg}, b_{hg}\\}$, and $\\{W_{io}, b_{io}, W_{ho}, b_{ho}\\}$ are the learnable weights and biases for computing the input, forget, cell, and output gates respectively.\n",
    "\n",
    "At each time step the LSTM takes as input the previous hidden state, the previous memory cell state and the embedding (features) associated with the current word and generates the new hidden states and the prediction of the next word. \n",
    "\n",
    "**Hint: Note that the recurrsion is around two variables, namely, $h_t$ and $c_t$. Hence you will need to keep track of two previous hidden states.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1276616913.py, line 36)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_3127/1276616913.py\"\u001b[0;36m, line \u001b[0;32m36\u001b[0m\n\u001b[0;31m    criterion = ?     # the loss function\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# myLSTM based language model\n",
    "class myLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
    "        \"\"\"\n",
    "        Input: \n",
    "            vocab_size: the number of unique words in your dataset \n",
    "            embed_size: the size of the feature vector assiciated with each word\n",
    "            hidden_size: the size of the hidden state features\n",
    "        \"\"\"\n",
    "        super(myLSTM, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size) # features associated with the words in your vocabulary\n",
    "        \n",
    "        # rest of your LSTM model code goes here\n",
    "        \n",
    "    def forward(self, x, h):\n",
    "        \"\"\"\n",
    "        Perform forward propagation over a single unit composed of an LSTM unit followed by a linear layer to \n",
    "        generate the input activations for the softmax function. \n",
    "    \n",
    "        Input:\n",
    "            x: the current input (indices into the vocabulary)\n",
    "            h: the previous hidden states\n",
    "\n",
    "        Output:\n",
    "            out: the output of the unit (which are the input activations of the softmax layer to predict the next word)\n",
    "            (h, c): the current hidden states composed of the hidden state and the memory cell state\n",
    "        \"\"\"\n",
    "        \n",
    "        # your code goes here\n",
    "        return out, (h, c)\n",
    "\n",
    "model = myLSTM(vocab_size, embed_size, hidden_size)\n",
    "\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = ?     # the loss function\n",
    "\n",
    "# we will use the Adam optimizer for faster and easier convergence\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "?? Use cross entropy loss function from pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop for the model (7.5 Points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Truncated backpropagation\n",
    "def detach(states):\n",
    "    # Detach from pytorch computational graph\n",
    "    return [state.detach() for state in states]\n",
    "\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    # Set initial hidden and cell states\n",
    "    states = (torch.zeros(1, batch_size, hidden_size).to(device),\n",
    "              torch.zeros(1, batch_size, hidden_size).to(device))\n",
    "    \n",
    "    for i in range(0, ids.size(1) - seq_length, seq_length):\n",
    "        # Get mini-batch inputs and targets\n",
    "        inputs = ids[:, i:i+seq_length].to(device)\n",
    "        targets = ids[:, (i+1):(i+1)+seq_length].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        # Starting each batch, we detach the hidden state from how it was previously produced.\n",
    "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
    "        states = detach(states)\n",
    "        \n",
    "        ## the code for the rest of the forward pass goes here ##\n",
    "        \n",
    "        \n",
    "        # Backward pass \n",
    "        \n",
    "        ## code for backward pass goes here ##\n",
    "\n",
    "        # we clip the gradients to ensure that they remain bounded. This is a hack! \n",
    "        clip_grad_norm_(model.parameters(), 0.5)\n",
    "        \n",
    "        # Optimize        \n",
    "        \n",
    "        ## code for optimization goes here ##\n",
    "\n",
    "        step = (i+1) // seq_length\n",
    "        if step % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step[{}/{}], Loss: {:.4f}, Perplexity: {:5.2f}'\n",
    "                   .format(epoch+1, num_epochs, step, num_batches, loss.item(), np.exp(loss.item())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing loop for the model (7.5 Points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Test the model\n",
    "states = (torch.zeros(1, batch_size, hidden_size).to(device),\n",
    "              torch.zeros(1, batch_size, hidden_size).to(device))\n",
    "test_loss = 0.\n",
    "with torch.no_grad():\n",
    "    for i in range(0, test_ids.size(1) - seq_length, seq_length):\n",
    "        # Get mini-batch inputs and targets\n",
    "        inputs = test_ids[:, i:i+seq_length].to(device)\n",
    "        targets = test_ids[:, (i+1):(i+1)+seq_length].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        states = detach(states)\n",
    "        \n",
    "        ## code for forward pass goes here ##\n",
    "\n",
    "test_loss = test_loss / test_num_batches\n",
    "print('test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
    "    test_loss, math.exp(test_loss)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating text using the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate texts using trained model\n",
    "with torch.no_grad():\n",
    "    with open('sample.txt', 'w') as f:\n",
    "        # Set intial hidden ane cell states\n",
    "        state = (torch.zeros(1, 1, hidden_size).to(device),\n",
    "                 torch.zeros(1, 1, hidden_size).to(device))\n",
    "\n",
    "        # Select one word id randomly\n",
    "        prob = torch.ones(vocab_size)\n",
    "        input = torch.multinomial(prob, num_samples=1).unsqueeze(1).to(device)\n",
    "\n",
    "        for i in range(num_samples):\n",
    "            # Forward propagate RNN \n",
    "            output, state = model(input, state)\n",
    "\n",
    "            # Sample a word id\n",
    "            prob = output.exp()\n",
    "            word_id = torch.multinomial(prob, num_samples=1).item()\n",
    "\n",
    "            # Fill input with sampled word id for the next time step\n",
    "            input.fill_(word_id)\n",
    "\n",
    "            # File write\n",
    "            word = corpus.dictionary.idx2word[word_id]\n",
    "            word = '\\n' if word == '<eos>' else word + ' '\n",
    "            f.write(word)\n",
    "\n",
    "            if (i+1) % 100 == 0:\n",
    "                print('Sampled [{}/{}] words and save to {}'.format(i+1, num_samples, 'sample.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As part of your solution please submit the sample.txt file as well. Name it as \\<your-netid>_sample_hw4.txt and include it in your top level zip file**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question P2: Ensemble of Neural Networks (35 Points Total)\n",
    "In this part of the assignment, you will train an ensemble of neural networks for classifying hand written digits in the MNIST dataset. You will take the code provided in the following cell and extend it to build an ensemble of single hidden layer MLP as per the specifications provided in the questions below. \n",
    "\n",
    "For the data, you need to download the MNIST dataset from ``http://yann.lecun.com/exdb/mnist/`` and place the files in the directory ``./data/MNIST``. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87163081161b4df4980ec3229385e234",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9912422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1adad4477e154f21b1a17a651af6dd12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06541d92db9a400f8d4cbcaaa706faa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1648877 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc423cb5998649b59248e38a856ed380",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4542 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'get_n_params' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5613/9902507.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;31m# initialize the optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_fnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmoment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Number of parameters: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_n_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_fnn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;31m# train the model for one epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_n_params' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "\n",
    "\n",
    "# load the MNIST dataset\n",
    "input_size  = 28*28   # images are 28x28 pixels\n",
    "output_size = 10      # there are 10 classes\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./data/MNIST', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=64, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./data/MNIST', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=1000, shuffle=True)\n",
    "\n",
    "\n",
    "# define the multi-layer perceptron model\n",
    "class FC2Layer(nn.Module):\n",
    "    def __init__(self, input_size, n_hidden, output_size):\n",
    "        super(FC2Layer, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_size, n_hidden), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(n_hidden, n_hidden), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(n_hidden, output_size), \n",
    "            nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.input_size)\n",
    "        return self.network(x)\n",
    "    \n",
    "\n",
    "# define the training loop\n",
    "def train(epoch, model):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass through the model\n",
    "        output = model(data)\n",
    "        # forward pass through the cross-entropy loss function\n",
    "        loss = F.nll_loss(output, target)\n",
    "        # backward pass through the cross-entropy loss function and the model\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "\n",
    "# define the testing loop\n",
    "def test(model):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        output = model(data)\n",
    "        test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss                                                               \n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability                                                                 \n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    accuracy_list = []\n",
    "    accuracy_list.append(accuracy)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        accuracy))\n",
    "\n",
    "    \n",
    "# build the model and execute the training and testing\n",
    "\n",
    "# initialize some hyper-paramters \n",
    "n_hidden = 4 # number of hidden units\n",
    "learning_rate = 0.01\n",
    "moment = 0.5\n",
    "nepochs = 1\n",
    "\n",
    "# build the actual model\n",
    "model_fnn = FC2Layer(input_size, n_hidden, output_size)\n",
    "#model_fnn.to(device)\n",
    "# initialize the optimizer\n",
    "optimizer = optim.SGD(model_fnn.parameters(), lr=learning_rate, momentum=moment)\n",
    "print('Number of parameters: {}'.format(get_n_params(model_fnn)))\n",
    "\n",
    "# train the model for one epoch\n",
    "for epoch in range(0, nepochs):\n",
    "    train(epoch, model_fnn)\n",
    "    test(model_fnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question P2.a: Ensemble of networks of same size but initialized with a different seed (20 Points)\n",
    "Extend the above code to create an ensemble of `k` single hidden layer MLP models, each initialized with a different random seed. Train each model inside the ensemble for exactly `1` epoch and report the performance of the individual models and the ensemble model on the test set. For the purpose of ensembling, you can take a majority vote of the model outputs. \n",
    "\n",
    "At a high level you need to perform the following tasks:\n",
    "1. Initialize `k` models with different random seed (for some value of k): (**5 points**)\n",
    "2. Extend the train() function to train individual models inside the ensemble: (**5 points**)\n",
    "3. Extend the test() function to estimate the accuracy of the individual models and the ensemble model: (**5 points**)\n",
    "4. Repeat the above process with different values of `k`. For this exercise use $k = \\{1, 2, 4, 8, 16, 32 \\}$, and plot a graph with `k` on the x-axis and ensemble model performance on the y-axis. (**5 points**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question P2.b: Bagged networks  (15 Points)\n",
    "Extend the above code to create a bag of `k` single hidden layer MLP models with each with `4` hidden units. Compare the performance of bags of different sizes `k`. In particular, compare the performance of `5` bages, with sizes $k = \\{2, 4, 8, 16, 32\\}$ respectively. To prevent additional sources of randomness, all the models within all the bags should be initialized to the same random seed. Train each model inside each bag for exactly `1` epoch. Report the final performance for each bag on the test set and compare it against each other. For the purpose of ensembling, you can take a majority vote of the model outputs. \n",
    "\n",
    "At a high level you need to perform the following tasks:\n",
    "1. Create `k=5` bags of the training set of sizes $k = \\{2, 4, 8, 16, 32\\}$. (**5 points**)\n",
    "2. Use the same extension to the train() function as in P2.a to train individual models in each bag: (**2.5 points**)\n",
    "3. Use the same extension to the test() function as in P2.a to estimate the accuracy for each bag: (**2.5 points**)\n",
    "4. Plot a graph with bag_size on the x-axis and bag performance on the y-axis. (**5 points**)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "374px",
    "left": "1310px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
