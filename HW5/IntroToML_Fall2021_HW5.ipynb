{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning (CSCI-UA.473)\n",
    "\n",
    "## Homework 5: Dimensionality Reduction and Clustering\n",
    "\n",
    "### Due: December 5th, 2021 at 11:59PM\n",
    "\n",
    "### Name: (your name goes here)\n",
    "\n",
    "### Email: (your NYU email goes here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework we will practice three concepts: \n",
    "1. Principal Component Analysis\n",
    "2. Non-negative Matrix Factorization\n",
    "3. K-means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "from sklearn.decomposition import PCA, NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting data (nothing to do here)\n",
    "\n",
    "**Please review the documentation for [TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) and [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) to see what these functions do.  Here is more information on the [dataset](https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html).**\n",
    "\n",
    "\n",
    "Recall that for dimensionality reduction we form the matrix\n",
    "\n",
    "$$\n",
    "{\\bf X} = [{\\bf x}_1; {\\bf x}_2; \\ldots; {\\bf x}_n] \\in \\mathbb{R}^{d\\times n}\n",
    "$$\n",
    "\n",
    "where ${\\bf x}_i \\in \\mathbb{R}^d$ is the space of the data and $n$ is the number of training points.  We want to find a **dictionary** matrix ${\\bf W}$ and a **code** matrix ${\\bf Z}$ such that ${\\bf X} \\approx {\\bf WZ}$.  Dimensionality reduction comes in when we select ${\\bf W} \\in \\mathbb{R}^{d \\times p}$ and ${\\bf Z} \\in \\mathbb{R}^{p\\times n}$ with $p \\ll d$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X has shape (d, n) =  (1771, 2369)\n"
     ]
    }
   ],
   "source": [
    "# Load in the data.\n",
    "newsgroups_train = datasets.fetch_20newsgroups(subset='train', \n",
    "                                               categories=['comp.sys.mac.hardware', 'rec.motorcycles', 'sci.med', 'soc.religion.christian'])\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df = 30)\n",
    "\n",
    "# The data matrix X and the targets y.\n",
    "X = (tfidf_vectorizer.fit_transform(newsgroups_train.data).T).todense()\n",
    "y = newsgroups_train.target\n",
    "\n",
    "(d, n) = X.shape\n",
    "print(\"X has shape (d, n) = \", X.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question P1: Principal Component Analysis (25 Points Total)\n",
    "\n",
    "Recall that for dimensionality reduction we form the matrix\n",
    "\n",
    "$$\n",
    "{\\bf X} = [{\\bf x}_1; {\\bf x}_2; \\ldots; {\\bf x}_n] \\in \\mathbb{R}^{d\\times n}\n",
    "$$\n",
    "\n",
    "where ${\\bf x}_i \\in \\mathbb{R}^d$ is the space of the data and $n$ is the number of training points.  We want to find a **dictionary** matrix ${\\bf W}$ and a **code** matrix ${\\bf Z}$ such that ${\\bf X} \\approx {\\bf WZ}$.  Dimensionality reduction comes in when we select ${\\bf W} \\in \\mathbb{R}^{d \\times p}$ and ${\\bf Z} \\in \\mathbb{R}^{p\\times n}$ with $p \\ll d$.\n",
    "\n",
    "The dimensions of the data loaded above is reduced from `d=1771` to `p=5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The reduced dimension.\n",
    "p = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions P1.a: Computing the dictionary and code matrices (5 Points)\n",
    "Using the PCA implemented in Sci-Kit learn (`PCA()`), obtain the low-dimensional projection of ${\\bf X}$ (i.e. get ${\\bf W}$ and ${\\bf Z}$) for when $p = 5$.  Read the documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) carefully.  In particular, make sure you are computing the correct ${\\bf W}$ and ${\\bf Z}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code goes here\n",
    "\n",
    "W = ?\n",
    "Z = ?\n",
    "\n",
    "print(\"W has shape \", W.shape)\n",
    "print(\"Z has shape \", Z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to check your results.  It prints out the first 5 principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also print out the first p components.  There is nothing to do for this part.\n",
    "loadings = W.T\n",
    "invocab = dict([(vv,kk) for kk, vv in tfidf_vectorizer.vocabulary_.items()])\n",
    "\n",
    "for i in range(q):\n",
    "    insens_idx = np.argsort(loadings[i])\n",
    "\n",
    "    print ('COMPONENT ' + str(i + 1))\n",
    "    print (' '.join([invocab[w_ix] for w_ix in insens_idx[-10:]]) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question P1.b: Plotting the reduced data (5 Points)\n",
    "\n",
    "The matrix ${\\bf Z} = [{\\bf z}_1; {\\bf z}_2; \\ldots ; {\\bf z}_n] \\in \\mathbb{R}^{p \\times n}$ is the matrix of the reduced data.  Make a scatter plot of the data projected onto its first two principal components.  In other words, form a scatter plot of all the ${\\bf z}_i$ with the first component as the $x$-axis and the second component as the $y$-axis.  Set the color bar so that the points are colored corresponding to their target label ${\\bf y}$ (which was defined earlier in the code).  Hint: look at the documentation [here](https://matplotlib.org/3.3.2/api/_as_gen/matplotlib.pyplot.scatter.html) for the `scatter()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "## Your code starts here\n",
    "\n",
    "\n",
    "## Your code ends here\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question P1.c: Explain the variance (15 Points)\n",
    "\n",
    "Recall that for PCA the dictionary and code matrices ${\\bf W},{\\bf Z}$ can be found using either an eigenvalue decomposition or the SVD.  Suppose the matrix ${\\bf X}$ is centered so that \n",
    "$$\n",
    "\\frac{1}{n}\\sum_{i = 1}^n {\\bf x}_i = \\vec{0} \\in \\mathbb{R}^d\n",
    "$$\n",
    "The covariance matrix is defined to be ${\\bf C} = {\\bf XX}^T$.  The eigen-decomposition of ${\\bf C} = {\\bf W\\Sigma W}^T$ gives the matrices ${\\bf W}$ and ${\\bf Z} = {\\bf \\Sigma W}^T$.  Here\n",
    "\n",
    "$$\n",
    "{\\bf \\Sigma} = \\begin{bmatrix}\n",
    "\\sigma_1^2 & & \\\\\n",
    " & \\ddots & \\\\\n",
    " & & \\sigma_d^2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "is the diagonal matrix with eigenvalues $\\sigma_i^2$.  The proportion of explained variance by our low-dimensional projection is defined to be\n",
    "\n",
    "$$\n",
    "\\mathrm{PV}(p) := \\frac{\\sum_{i=1}^p \\sigma_i^2}{\\sum_{i=1}^d \\sigma_i^2}\n",
    "$$\n",
    "\n",
    "Make a plot of $\\mathrm{PV}(p)$ versus $p$ for $p = 1,\\ldots,1000$.  You do not need to implement the procedure above on your own, but can instead use the Sci-kit learn PCA.  In particular, look at the `explained_variance_ratio_` attribute.  Make sure that your plot has an appropriate title and labels for the axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code goes here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question P2. Non-Negative Matrix Factorization (NMF) (30 Points Total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question P2.a: Computing the dictionary and code matrices (10 Points)\n",
    "\n",
    "Similarly to PCA in Part 1, use the NMF implemented in Sci-Kit learn to obtain the low-dimensional projection of ${\\bf X}$ (i.e. get ${\\bf W}$ and ${\\bf Z}$) for when $p = 5$.  Read the documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html) carefully.  In particular, make sure you are computing the correct ${\\bf W}$ and ${\\bf Z}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code goes here\n",
    "\n",
    "W = ?\n",
    "Z = ?\n",
    "\n",
    "print(\"W has shape \", W.shape)\n",
    "print(\"Z has shape \", Z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question P2.b: Compute the reconstruction error (20 Points)\n",
    "\n",
    "Define the reconstruction error\n",
    "\n",
    "$$\n",
    "R(p) := \\frac{1}{2}\\|{\\bf X} - {\\bf WZ}\\|_F^2\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\|M\\|_F^2 = \\sum_{i = 1}^m \\sum_{j = 1}^k M_{ij}^2\n",
    "$$\n",
    "\n",
    "is the Frobenius norm of $M \\in \\mathbb{R}^{m \\times k}$.  Make a plot of the reconstuction error $R(p)$ versus $p$, for $p = 5, 10, 15, \\ldots, 50$.  Make sure the plot is clear with labeled axes and an appropriate title.  You can just use the `reconstruction_err_` attribute provided in Sci-kit learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question P3. K-Means Clustering (45 Points Total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now learn about k-means algorithm and use it for the problem of image segmentation. In particular, you will use the K-means clustering algorithm to segment an image into $K$ distinct colors.  The file name for the particular image you will work with is `wsq_park.jpg` (provided to you as part of the assignment). \n",
    "\n",
    "\n",
    "We first need to install a couple of necessary packages used for the purpose of computer vision. \n",
    "\n",
    "Below are all of the packages you will need for this assignment.  `cv2` refers to the package OpenCV, which you may need to install.  This can be done with the command\n",
    "\n",
    "`pip install opencv-python`\n",
    "\n",
    "OpenCV is a package for computer vision, but here we will only use it to help display images.  You may also need the `pickle` package which can be installed with\n",
    "\n",
    "`pip install pickle`\n",
    "\n",
    "**Do not** alter the cell below or add any other packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from scipy.stats import multivariate_normal as normal # Multivariate normal distribution\n",
    "from scipy.stats import multinomial # Multinomial distribution\n",
    "import cv2 # OpenCV package for computer vision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the image data (nothing to do here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the image and convert it from BGR (used by cv2) to RGB (used by matplotlib).\n",
    "wsq_image = cv2.cvtColor(cv2.imread(\"wsq_park.jpg\"), cv2.COLOR_BGR2RGB)\n",
    "plt.figure(figsize = (15,10))\n",
    "# Show the image.\n",
    "plt.imshow(wsq_image);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since each pixel has a red, blue, and green value we can think of each pixel as a vector ${\\bf x}_i \\in \\mathbb{R}^3$ for $i=1,\\ldots,N$, where $N$ is the number of pixels in the image.  Let ${\\bf X} = [{\\bf x}_1,\\ldots, {\\bf x}_N]^T \\in \\mathbb{R}^{N \\times 3}$ be the dataset which is the collection of all of the pixels.  You will use K-means to cluster the points in the dataset together, which will effectively segment into regions of similar colors.  The code below processes the image for you to fit this format.  You do not need to do anything here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset X of all the pixel RGB values.\n",
    "X = wsq_image.reshape((wsq_image.shape[0] * wsq_image.shape[1], wsq_image.shape[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question P3.a: Implementing K-means (25 Points)\n",
    "\n",
    "Start by implementing the `KMeans` class.  You need to fill in the following methods.\n",
    "\n",
    "1. `fit`, the main K-means clustering algorithm to determine the cluster centers $\\mu_j \\in \\mathbb{R}^3$ for $j = 1,\\ldots,K$.\n",
    "2. `compute_distances`, a method to compute the squared distances $\\|{\\bf x}_i - \\mu_j\\|^2$ for all $i=1,\\ldots,N$ and $j = 1,\\ldots,K$.\n",
    "3. `distortion`, a method that computes the distortion $J$ defined by\n",
    "$$\n",
    "J = \\sum_{i=1}^N \\sum_{j=1}^K r_{ij} \\|{\\bf x}_i - \\mu_j\\|^2\n",
    "$$\n",
    "where\n",
    "$$\n",
    "r_{ij} = \\begin{cases}\n",
    "1 & \\text{ if } \\|{\\bf x}_i - \\mu_j\\|^2 = \\mathrm{min}_{l \\in \\{1,\\ldots, K\\}} \\|{\\bf x}_i - \\mu_l\\|^2\\\\\n",
    "0 & \\text{ otherwise }\n",
    "\\end{cases}\n",
    "$$\n",
    "In other words, $r_{ij}$ is 1 if data point ${\\bf x}_i$ is assigned to cluster $j \\in \\{1,\\ldots,K\\}$ and 0 otherwise.\n",
    "\n",
    "See the methods below for further description.  You may use the Numpy function `numpy.linalg.norm` to compute the norms of vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeans:\n",
    "    \n",
    "    \"\"\"\n",
    "    Initialize the K-means model.  Here labels denotes the numpy array of shape (N, ) where each entry at index\n",
    "    i is an integer in {1,...,K} according to which cluster point x_i is assigned to.\n",
    "    \n",
    "    Input:\n",
    "    K, int -- the number of clusters\n",
    "    X, numpy.ndarray -- shape (N, d) where N is the number of data points and d is the dimension\n",
    "    mu_0, numpy.ndarray -- shape (K, d) where K is number of clusters and d is the dimension, \n",
    "                           initialization of cluster centers\n",
    "    \"\"\"\n",
    "    def __init__(self, K, X, mu_0):\n",
    "        assert K == mu_0.shape[0]\n",
    "        self.K = K\n",
    "        self.X = X\n",
    "        self.N = X.shape[0]\n",
    "        self.mu = mu_0\n",
    "        self.labels = None\n",
    "    \n",
    "    \"\"\"\n",
    "    The K-Means clustering algorithm.  Update the array of the means, m, as well as the array of labels\n",
    "    for each point in the dataset.\n",
    "    \n",
    "    Input:\n",
    "    iters, int -- the number of K-means iterations to perform\n",
    "    \n",
    "    Return:\n",
    "    labels, numpy.ndarray -- numpy array of ints of shape (N, )\n",
    "                             the cluster assignments (i.e. self.labels after it has been updated)\n",
    "    \"\"\"\n",
    "    def fit(self, iters):\n",
    "        labels = np.zeros(self.N)\n",
    "        mu = self.mu\n",
    "        ## TO DO STARTS HERE ##\n",
    "\n",
    "        ## TO DO ENDS HERE ##\n",
    "        \n",
    "        # Ensure that the cluster assignments are integers.\n",
    "        labels = labels.astype(int)\n",
    "        self.labels = labels\n",
    "        self.mu = mu\n",
    "        return labels\n",
    "    \n",
    "    \"\"\"\n",
    "    Compute the squared distances from each point in the dataset to each cluster center in the array m.\n",
    "    \n",
    "    Return:\n",
    "    dist, numpy.ndarray -- shape (N, K) of |x_i - mu_j|^2 for i=1,...,N and j=1,...,K\n",
    "    \"\"\"\n",
    "    def compute_distances(self):\n",
    "        ## TO DO STARTS HERE ##\n",
    "        dist = ?\n",
    "        ## TO DO ENDS HERE ##\n",
    "        return dist\n",
    "    \n",
    "    \"\"\"\n",
    "    Compute the distortion J for the current clustering assignment.  Assume that the cluster assignments have\n",
    "    already been computed after fit is called with at least 1 iteration.\n",
    "    \n",
    "    Return:\n",
    "    J, float -- the distortion\n",
    "    \"\"\"\n",
    "    def distortion(self):\n",
    "        assert self.labels is not None   \n",
    "        J = 0.\n",
    "        ## TO DO STARTS HERE ##\n",
    "        \n",
    "        ## TO DO ENDS HERE ##\n",
    "        return J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question P3.b: Distortion and the Elbow Method (10 Points)\n",
    "\n",
    "In this part you will determine the number of clusters to use for image segmentation by plotting the distortion $J$ for different $K$.  You must do the following.\n",
    "\n",
    "1.  Use 15 iterations of K-means and compute the distortion for $K = 1,2,\\ldots,9$.\n",
    "2.  Approximately how many clusters should be used?  In other words, what value of $K$ corresponds to the \"elbow\" in the curve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the initial means for K-means.\n",
    "initial_means = pickle.load(open(\"kmeans_initial_means.p\", \"rb\"))\n",
    "\n",
    "K = np.arange(1, 9 + 1, 1) # The array of K values, k = 1,2,...,9.\n",
    "J = np.zeros(len(K)) # The array of the distortion values for the different k.\n",
    "\n",
    "# Loop over all k = 1,2,...,9\n",
    "for k in range(1, len(K) + 1):\n",
    "    \n",
    "    # Initialization of the cluster centers.\n",
    "    mu_0 = initial_means[k - 1]\n",
    "    \n",
    "    # Compute the distortion.\n",
    "    ## TO DO STARTS HERE ##\n",
    "\n",
    "    ## TO DO ENDS HERE ##\n",
    "    print('K = {:d} finished.'.format(k))\n",
    "\n",
    "    \n",
    "# Plot the results.\n",
    "plt.plot(K, J, 'b-s', lw = 2)\n",
    "plt.xlabel(r'Clusters $k$')\n",
    "plt.ylabel(r'Distortion')\n",
    "plt.title(r'Elbow Method for $K$-means')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question P3.c: Image Segmentation (10 Points)\n",
    "\n",
    "Now use the $K$ you found in the previous part from the elbow method to cluster the dataset ${\\bf X}$.  You will plot the segmented image, which now only uses $K$ distinct colors.  To do this you will need to do the following.\n",
    "\n",
    "1.  Run K-means for 15 iterations using the $K$ you found in the previous part.  You may use any initialization you wish as long as your results converge.  Although you may want to use the provided initialization.\n",
    "\n",
    "2.  Using the cluster assignments for each point create a new matrix ${\\bf Y} \\in \\mathbb{R}^{N \\times 3}$ where each row ${\\bf y}_i \\in \\mathbb{R}^3$ is $\\mu_j$, where $\\mu_j$ is the center of the cluster $j$ that ${\\bf x}_i$ is assigned to.  For example, if ${\\bf x}_1$ is assigned to cluster 2, then the first row of ${\\bf Y}$ is ${\\bf y}_1 = \\mu_2$.\n",
    "\n",
    "3.  Since RGB values are integers between 0 and 255 (inclusive) you will need to do one of two things.  You can either round all of the results in ${\\bf Y}$ to integers between 0 and 255, or you can normalize so that all values are floats between 0 and 1.  The `imshow` function will work with either of these.\n",
    "\n",
    "Note that choosing a larger $K$ will result in an image closer to the original, so if the $K$ you found in the previous part is small then the segmented image will only use a few different colors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO DO STARTS HERE ##\n",
    "Y = ?\n",
    "## TO DO ENDS HERE ##\n",
    "segmented_image = Y.reshape(wsq_image.shape)\n",
    "\n",
    "plt.figure(figsize = (15, 10))\n",
    "plt.imshow(segmented_image)\n",
    "plt.show();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
