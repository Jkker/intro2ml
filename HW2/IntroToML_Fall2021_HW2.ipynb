{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning (CSCI-UA.473)\n",
    "\n",
    "## Homework 2: Analysis of Binary Classifiers and Linear Support Vector Machines\n",
    "### Due: October 12th, 2021 at 11:59PM\n",
    "\n",
    "\n",
    "### Name: Jerry Jia\n",
    "### Email: tj1043"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This homework has two parts. In the first part you will do some error analysis of a binary classifier. In the second part you will implement a dual form of a linear support vector machine without the optimizing algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question P1: Metrics for a binary classifier (20 Points Total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem you will convert your logistic regression model into a binary classifier by learning an `operating point` on the ROC curve of your model. In addition you will also compute and report certain error metrics on your binary classifier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a logistic regression model for Breast Cancer Classification using Sklearn (do nothing here)\n",
    "\n",
    "Here we first train a logistic regression model for the problem of Breast Cancer Classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision = 98.5%\n",
      "Recall    = 94.3%\n"
     ]
    }
   ],
   "source": [
    "# Load the packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "from sklearn.datasets import make_blobs, load_breast_cancer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "\n",
    "# General helper method to convert sci-kit datasets to Pandas DataFrame.\n",
    "def sklearn_to_df(sklearn_dataset):\n",
    "    df = pd.DataFrame(sklearn_dataset.data, columns=sklearn_dataset.feature_names)\n",
    "    df['target'] = pd.Series(sklearn_dataset.target)\n",
    "    return df\n",
    "\n",
    "cancer_dataset = load_breast_cancer() # Load the data and convert to a pandas dataframe\n",
    "df = sklearn_to_df(cancer_dataset)\n",
    "\n",
    "# Split the data.  DO NOT TOUCH THE TEST DATA FROM HERE ON!!\n",
    "train_data, val_data = model_selection.train_test_split(df, test_size = 0.2) # 0.2 is 20% validation data.\n",
    "\n",
    "# Split the features from the class labels.\n",
    "X_train = train_data.drop('target', axis = 1) # We drop the target from the features.\n",
    "X_val  = val_data.drop('target', axis = 1)  # Note that this does not operate inplace.\n",
    "\n",
    "y_train = train_data['target']\n",
    "y_val  = val_data['target']\n",
    "\n",
    "\n",
    "# Now fit a logistic regression model.\n",
    "model = LogisticRegression(solver = 'liblinear', class_weight = 'balanced')\n",
    "model.fit(X_train, y_train);\n",
    "\n",
    "# get the default predictions on the validation set\n",
    "val_pred = model.predict(X_val)\n",
    "\n",
    "# Precision score = tp / (tp + fp) => ability not to label a negative sample positive.\n",
    "precision = metrics.precision_score(y_val, val_pred)\n",
    "print(\"Precision = {:0.1f}%\".format(100 * precision))\n",
    "\n",
    "# Recall score = tp / (tp + fn) => ability to classify positive samples.\n",
    "recall = metrics.recall_score(y_val, val_pred)\n",
    "print(\"Recall    = {:0.1f}%\".format(100 * recall))\n",
    "\n",
    "# get the values of the probabilities generated by the logistic regression\n",
    "val_prob = model.predict_proba(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P1.a: Plot the ROC curve (10 Points)\n",
    "\n",
    "\n",
    "True Positive Rate (TPR) (also called Recall) is defined as: \n",
    "$$\n",
    "\\text{TPR} = \\frac{TP}{TP + FN}\n",
    "$$\n",
    "\n",
    "where $TP$ = number of true positive examples and $FN$ = number of false negative examples. An example is $TP$ if the models says `yes` and the true answer is `yes`. An example is $FN$ if the model says `no` but the true answer is `yes`. \n",
    "\n",
    "False Positive Rate (FPR) is defined as follows:\n",
    "$$\n",
    "\\text{FPR} = \\frac{FP}{FP + TN}\n",
    "$$\n",
    "\n",
    "where $FP$ = number of false positive examples and $FP$ = number of false negative examples. An example is $FP$ if the models says `yes` but the true answer is `no`. An example is $TN$ if the model says `no` and the true answer is `no`. \n",
    "\n",
    "An ROC curve plots TPR (y-axis) vs. FPR (x-axis) at all classification thresholds. Lowering the classification threshold classifies more items as positive, thus increasing both False Positives and True Positives. \n",
    "\n",
    "See this for more details (https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc)\n",
    "\n",
    "Plot the ROC curve on the validation set for the above problem. **Note that you are not allowed to use any library function to compute the ROC. You have to do it from scratch.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code to compute and plot ROC goes here\n",
    "def compute_tpr_fpr(val_prob, y_val, threshold):\n",
    "    y_val_pred = (val_prob[:, 1] >= threshold).astype(int)\n",
    "\n",
    "    tn, fp, fn, tp = 0, 0, 0, 0\n",
    "    for predicted, actual in zip(y_val_pred, y_val):\n",
    "        if predicted == actual == 1:\n",
    "            tp += 1\n",
    "        elif predicted == actual == 0:\n",
    "            tn += 1\n",
    "        elif predicted == 1 and actual == 0:\n",
    "            fp += 1\n",
    "        elif predicted == 0 and actual == 1:\n",
    "            fn += 1\n",
    "\n",
    "    tpr = tp / (tp + fn)\n",
    "    fpr = fp / (fp + tn)\n",
    "    return tpr, fpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fpr_list, tpr_list = [], []\n",
    "n_threshold = 1000\n",
    "for i in range(0, n_threshold + 1):\n",
    "    threshold = i / n_threshold\n",
    "    tpr, fpr = compute_tpr_fpr(val_prob, y_val, threshold)\n",
    "    tpr_list.append(tpr)\n",
    "    fpr_list.append(fpr)\n",
    "\n",
    "# plot the ROC curve\n",
    "plt.plot(fpr_list, tpr_list)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate using roc_curve method\n",
    "# fpr_list_1, tpr_list_1, thresholds = metrics.roc_curve(y_val, val_prob[:, 1])\n",
    "# plt.plot(fpr_list_1, tpr_list_1)\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.title('ROC Curve')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P1.b: Compute the AUC of ROC (10 Points)\n",
    "\n",
    "AUC stands for \"Area under the ROC Curve.\" That is, AUC measures the entire two-dimensional area underneath the entire ROC curve (think integral calculus) from (0,0) to (1,1). AUC provides an aggregate measure of performance across all possible classification thresholds. One way of interpreting AUC is as the probability that the model ranks a random positive example more highly than a random negative example. \n",
    "\n",
    "Consider the following example in which the predictions are sorted from left to right in ascending order of logistic regression probabilities for one class (say class 1): \n",
    "$$\n",
    "\\begin{align*}\n",
    "&  & 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 1 1 1 1 \\\\ \n",
    "Prob: & 0.0 \\rightarrow &  1.0\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Positive and negative examples ranked in ascending order of logistic regression score. AUC represents the probability that a random positive example (1) is positioned to the right of a random negative example (0). AUC ranges in value from 0 to 1. A model whose predictions are 100% wrong has an AUC of 0.0; one whose predictions are 100% correct has an AUC of 1.0.\n",
    "\n",
    "AUC is desirable for the following two reasons:\n",
    "\n",
    "* AUC is scale-invariant. It measures how well predictions are ranked, rather than their absolute values.\n",
    "* AUC is classification-threshold-invariant. It measures the quality of the model's predictions irrespective of what classification threshold is chosen.\n",
    "\n",
    "Compute the AUC of your logistic regression model. **Note that you are not allowed to use any library function to compute the AUC. You have to do it from scratch.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC = 99.74%\n"
     ]
    }
   ],
   "source": [
    "## Your code to compute the AUC goes here\n",
    "auc = 0\n",
    "for i in range(0, n_threshold - 1):\n",
    "    auc += (fpr_list[i] - fpr_list[i + 1]) * (\n",
    "        tpr_list[i + 1] + tpr_list[i]) / 2\n",
    "\n",
    "print(f'AUC = {auc*100:.2f}%')\n",
    "\n",
    "# Validate using roc_auc_score method\n",
    "# metrics.roc_auc_score(y_val, val_prob[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question P2: Linear Support Vector Machines (50 Points Total)\n",
    "\n",
    "In this problem you will implement a linear SVM by solving the dual problem and apply it to the Iris dataset (https://archive.ics.uci.edu/ml/datasets/iris). The original dataset has $4$ features with $3$ classes, but in this homework we'll only use the first two features and ignore the first class in order to make this a binary classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need the following packages below to do the homework.  Please DO NOT import any other packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from scipy.optimize import minimize\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and splitting the data (nothing to do)\n",
    "\n",
    "The following cell loads the data and pre-processes it to be used for training later.  **Do not modify anything in this cell**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the data.\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Ignore the first class and use only the first 2 features.\n",
    "X = X[y != 0, :2]\n",
    "y = y[y != 0]\n",
    "\n",
    "# Make sure that the class labels are either +1 or -1.\n",
    "y[y==2] = -1\n",
    "\n",
    "n_sample = len(X)\n",
    "\n",
    "# Randomly order the data.\n",
    "np.random.seed(0)\n",
    "order = np.random.permutation(n_sample)\n",
    "X = X[order]\n",
    "y = y[order].astype(float)\n",
    "\n",
    "# Split the data into 10% testing and 90% training.\n",
    "X_train = X[:int(.9 * n_sample)]\n",
    "y_train = y[:int(.9 * n_sample)]\n",
    "X_test = X[int(.9 * n_sample):]\n",
    "y_test = y[int(.9 * n_sample):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P2.a:  Implement the function to compute the inner product matrix (5 Points)\n",
    "\n",
    "For training data $\\mathcal{D} = \\{x_1, x_2, \\ldots, x_n\\}$, where $x_i \\in \\Re^p \\, \\forall i$, the inner product matrix $M \\in \\Re^{n \\times n}$ is defined by \n",
    "\n",
    "$$\n",
    "M = \\begin{bmatrix}\n",
    "m_{1,1} & m_{1,2} & \\cdots & m_{1,n}\\\\\n",
    "m_{2,1} & m_{2,2} & \\cdots & m_{2,n}\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "m_{n,1} & m_{n,2} & \\cdots & m_{n,n}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where $m_{i,j} = \\langle x_i, x_j\\rangle \\in \\Re$ is the inner product of the vectors $x_i$ and $x_j$. \n",
    "\n",
    "Implement the function below that takes in the data matrix $X$ of training data and returns the corresponding inner product matrix $M$. You may use the numpy function `np.dot()`.  Note that the data matrix $X \\in \\Re^{n \\times p}$ is a matrix whose $i^{th}$ row is the input sample $x_i$. $p$ is the dimension of each input $x_i$. \n",
    "\n",
    "Also answer the following two questions below.\n",
    "\n",
    "1. What is the fewest number of inner products do you need to compute?  Explain why.\n",
    "\n",
    "2. What are the diagonal entries of the matrix $M$?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inner_product_matrix(X):\n",
    "    \"\"\"\n",
    "    Compute the inner product matrix of the training data X where each row is a different data point x_i.\n",
    "    \n",
    "    Input:\n",
    "        X: np.ndarray(n, p), n data points in dimension of p\n",
    "    \n",
    "    Return:\n",
    "        M: np.ndarray(n, n), each entry is the inner product of the corresponding pair of vectors m_{ij}\n",
    "    \"\"\"\n",
    "    ##TODO-start##\n",
    "    n = X.shape[0]\n",
    "\n",
    "    M = np.ndarray(shape=(n, n))\n",
    "    for i in range(n):\n",
    "        for j in range(i, n):\n",
    "            M[i, j] = np.dot(X[i], X[j])\n",
    "            if not i == j:\n",
    "                M[j, i] = M[i, j]\n",
    "\n",
    "    return M\n",
    "    ##TODO-end##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answers to the two theoretical questions goes here.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What is the fewest number of inner products do you need to compute?  Explain why.\n",
    "\n",
    "    Answer: Since the inner product matrix is symmetric and $\\langle x_i, x_j\\rangle = \\langle x_j, x_i\\rangle$ by transitivity , we only need to compute $\\binom{n}2 = \\frac{n(n - 1)}{2}$ inner products \n",
    "\n",
    "\n",
    "2. What are the diagonal entries of the matrix $M$?\n",
    "\n",
    "    Answer: $\\langle x_i, x_i\\rangle = ||x_i||^2$ (squared L2 norm of D's entries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P2.b: Implement the objective function of the dual form of linear SVM (10 Points)\n",
    "\n",
    "Recall that for linear SVM the dual problem is given by \n",
    "\n",
    "\\begin{align*}\n",
    "\\max_{\\alpha} W(\\alpha) &= \\sum_{i=1}^n \\alpha_i - \\frac{1}{2}\\sum_{i=1}^{n}\\sum_{j=1}^{n} y_i y_j \\alpha_i \\alpha_j \\langle x_i, x_j\\rangle \\\\\n",
    "\\text{s.t.} & \\quad 0 \\le \\alpha_i \\le C, \\quad i = 1,\\ldots,n \\\\\n",
    "& \\sum_{i=1}^n \\alpha_i y_i = 0\n",
    "\\end{align*}\n",
    "\n",
    "where $\\alpha \\in \\mathbb{R}^n$ is a vector.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the objective function $W(\\alpha)$ which also takes the training data features and labels as parameters.  Do not worry about the constraints for the moment since you will deal with these next.\n",
    "\n",
    "* You may also find the function `np.diag()` useful to help vectorize your code and make it run faster.\n",
    "* Instead of writing your own optimization algorithm, use the scipy function `scipy.optimize.minimize()` to automatically optimize $\\alpha$. (Reference: https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html) \n",
    "* Note that the `minimize()` function minimizes the objective function, so you'll need to reformulate the dual problem as a minimization problem with the following trick.\n",
    "$$\n",
    "\\alpha^* = \\text{argmax}_{\\alpha}[W(\\alpha)]=\\text{argmin}_{\\alpha}[-W(\\alpha)].\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_function(a, X, y):\n",
    "    \"\"\"\n",
    "    The objective function of the dual problem W.\n",
    "    \n",
    "    Input:\n",
    "        a: np.ndarray(n,), the parameter alpha we want to optimize\n",
    "        X: np.ndarray(n, p), the matrix of training data features\n",
    "        y: np.ndarray (n,), the vector of training data labels, must be either +1 or -1.\n",
    "    \n",
    "    Return:\n",
    "        W: float, value of the objective function. \n",
    "    \"\"\"\n",
    "    \n",
    "    ##TODO-start## \n",
    "    W = ?\n",
    "    ##TODO-end##\n",
    "    return W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P2.c: Implementing the `fit()` function (30 Points)\n",
    "\n",
    "There are several pieces you will need to implement.  Read the documentation carefully to set everything up correctly.\n",
    "\n",
    "1. Get the box constraints $0 \\le \\alpha_i \\le C$ for $i=1,\\ldots,n$.  You will pass this into `minimize()` as the `bounds` argument.\n",
    "\n",
    "2. Get the linear constraint $\\sum_{i=1}^n \\alpha_i y_i = 0$.  You will pass this into `minimize()` as the `constraints` argument.\n",
    "\n",
    "3. Call `minimize()` using the correct objective function $-W(\\alpha)$ as well as the constraints and bounds from the previous 2 parts.  Use $\\alpha_0 = 0 \\in \\mathbb{R}^n$ as the initial point and use the SLSQP method.\n",
    "\n",
    "4. Compute the primal variable $w \\in \\mathbb{R}^p$ using the formula\n",
    "\n",
    "$$\n",
    "    w = \\sum_{i=1}^n \\alpha_i y_i x_i\n",
    "$$\n",
    "\n",
    "4. Compute the bias term using the formula\n",
    "\n",
    "$$\n",
    "     b = \\frac{1}{n} \\sum_{i=1}^{n}\\left(y_i - \\sum_{j=1}^n \\alpha_j y_j \\langle x_j, x_i \\rangle \\right) = \\frac{1}{n} \\sum_{i=1}^n \\left(y_i - w^T x_i \\right)\n",
    "$$\n",
    "\n",
    "once you have computed the minimizer $\\alpha$.\n",
    "\n",
    "\n",
    "Hints:\n",
    "* Read the explanations of `fun`, `x0`, `bounds`, `constraints`, corresponding to objective function, initialization, bounds, and constraints.\n",
    "* Equality constraints mean that the constraint function result is zero.\n",
    "* If `res = minimize(...)`, then `res.x` is the minimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(X, y, C):\n",
    "    \"\"\"\n",
    "    Computes the parameters alpha and bias that determine the maximum-margin decision boundary for SVM.\n",
    "    \n",
    "    Input:\n",
    "        X: np.ndarray(n,p), matrix of training data features\n",
    "        y: np.ndarray(n, ), vector of training data labels\n",
    "        C: float, slack parameter that is non-negative\n",
    "        \n",
    "    Return:\n",
    "        w: np.ndarray(p,), vector of primal variable values (vector orthogonal to decision boundary)\n",
    "        bias: float, the bias term in SVM\n",
    "        alpha: np.ndarray(n, ), vector of dual variable values\n",
    "    \"\"\"\n",
    "    ## TO-DO STARTS HERE##\n",
    "    alpha = ?\n",
    "    \n",
    "    # Compute the primal variables w.\n",
    "    w = ?\n",
    "    \n",
    "    # Compute the bias.\n",
    "    bias = ?\n",
    "    \n",
    "    ## TO-DO ENDS HERE ##\n",
    "    return (w, alpha, bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the results (nothing to do here)\n",
    "Please DO NOT change anything here. This may take a few minutes to finish the optimization.\n",
    "\n",
    "* If your implementation is perfect there won't be any errors thrown and it will show a figure similar to the first one in https://scikit-learn.org/stable/auto_examples/exercises/plot_iris_exercise.html#sphx-glr-auto-examples-exercises-plot-iris-exercise-py (ignore if the colors are swapped)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, alpha, bias = fit(X_train, y_train, C = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "plt.clf()\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, zorder=10, cmap=plt.cm.Paired,\n",
    "            edgecolor='k', s=20)\n",
    "\n",
    "# Circle out the test data\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], s=80, facecolors='none',\n",
    "            zorder=10, edgecolor='k')\n",
    "\n",
    "plt.axis('tight')\n",
    "x_min = X[:, 0].min()\n",
    "x_max = X[:, 0].max()\n",
    "y_min = X[:, 1].min()\n",
    "y_max = X[:, 1].max()\n",
    "\n",
    "XX, YY = np.mgrid[x_min:x_max:200j, y_min:y_max:200j]\n",
    "XXYY = np.c_[XX.ravel(), YY.ravel()]\n",
    "ZZ = []\n",
    "for i in range(XXYY.shape[0]):\n",
    "    ZZ.append(XXYY[i]@w+bias)\n",
    "    \n",
    "Z = np.array(ZZ)\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(XX.shape)\n",
    "plt.pcolormesh(XX, YY, Z > 0, cmap=plt.cm.Paired)\n",
    "plt.contour(XX, YY, Z, colors=['k', 'k', 'k'],\n",
    "            linestyles=['--', '-', '--'], levels=[-.5, 0, .5])\n",
    "plt.title('Linear SVM')\n",
    "plt.xlabel(r'$x_1$')\n",
    "plt.ylabel(r'$x_2$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P2.d: Implement the `predict()` function (5 Points)\n",
    "\n",
    "Implement the `predict()` function which computes the discriminant function on the test data and returns a vector whose entries are either $+1$ or $-1$.  Use the primal variable $w$ along with the bias $b$.  You **do not** need to modify the `accuracy()` function or other code in this cell.  If your method is correct you should achieve around 70% accuracy on both training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x_test, w, bias):\n",
    "    \"\"\"\n",
    "    Compute the predictions y_pred on the test set using only the support vectors.\n",
    "    \n",
    "    Input:\n",
    "        x_test: np.ndarray(n,p), matrix of the test data\n",
    "        alpha: np.ndarray(n,), vector of the dual variables\n",
    "        bias: float, the bias term\n",
    "    \n",
    "    Output:\n",
    "        y_pred: np.ndarray(n,), vector of the predicted labels, either +1 or -1\n",
    "    \"\"\"\n",
    "    ##TODO-start##\n",
    "    y_pred = ?\n",
    "    return y_pred\n",
    "    ##TODO-end##\n",
    "\n",
    "\n",
    "    \n",
    "def accuracy(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    Computes the accuracy on the test set given the class predictions.\n",
    "    \n",
    "    Input:\n",
    "        y_pred: np.ndarray(n,), vector of predicted class labels\n",
    "        y_true: np.ndarray(n,), vector of true class labels\n",
    "    \n",
    "    Output:\n",
    "        float, accuracy of predictions\n",
    "    \"\"\"\n",
    "    return np.mean(y_pred*y_true > 0)\n",
    "\n",
    "y_pred = predict(X_test, w, bias)\n",
    "y_pred_train = predict(X_train, w, bias)\n",
    "print(\"Training accuracy = {:0.2f}%\".format(100*accuracy(y_pred_train, y_train)))\n",
    "print(\"Testing accuracy = {:0.2f}%\".format(100*accuracy(y_pred, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "68ee14b412fd9617a94613e37903c7ba6bb9a423b3eb71728b972aa4f5f2b363"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('ml': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "374px",
    "left": "1310px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
