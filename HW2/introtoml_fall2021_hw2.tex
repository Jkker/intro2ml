\documentclass[10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{mhchem}
\usepackage{stmaryrd}
\usepackage{physics}

\title{Introduction to Machine Learning (CSCI-UA.473): Homework 2 }


\author{Instructor: Sumit Chopra}
\date{\today}


\begin{document}
\maketitle

\section*{Submission Instructions}
You must typeset the answers using IATEXand compile them into a single PDF file. Name the pdf file as: $\langle$ Your-NetID $\rangle$-hw2.pdf. For the programming part of the assignment, complete the Jupyter notebook named HW2.ipynb. Create a ZIP file containing both the PDF file and the completed Jupyter notebook. Name it $\langle \text{ Your-NetID }\rangle$-hw2.zip. Submit the ZIP file on Brightspace. The due date is October $12^{\text {th }}, \mathbf{2 0 2 1}$, 11:59 PM.

\section*{Theory}
\subsection*{Question T1: Model Selection (5 points)}
Consider that we are learning a logistic regression $M^{1}$ and a support vector machine $M^{2}$, and we have partitioned the data into three subsets: $D_{\text {train }}$ (training set), $D_{\text {val }}$ (validation set), and $D_{\text {test }}$ test set. The two models are iteratively optimized on $D_{\text {train }}$ over $T$ steps, and now we have $T$ logistic regression parameter configurations (i.e., weights and biases) $M_{1}^{1}, M_{2}^{1}, \ldots, M_{T}^{1}$ and $T$ support vector configurations $M_{1}^{2}, M_{2}^{2}, \ldots, M_{T}^{2}$ all with different parameters. We now evaluate the expected cost for all the $2 T$ models on training set, validation set, and test set. Thus we have $6 T$ quantities $\mathcal{L}_{\text {train }, t}^{i}, \mathcal{L}_{v a l, t}^{i}$, and $\mathcal{L}_{\text {test}, t}^{i}$ where $i \in\{1,2\}$ and $t \in\{1,2, \ldots, T\}$

\begin{enumerate}
  \item Which $i$ and $t$ should we pick as the best model and why? (2.5 points)

  \paragraph{Answer:} The best model is the one with the lowest weights and biases. For small values of $i$ (at the beginning of the fitting), the model will have higher validation error; while with too many iterations, the model will have higher generalization error due to over-fitting.

  \item How should we report the generalization error of the model? ( $2.5$ points)

  \paragraph{Answer:} For each model $M^{i}$ where $i \in\{1,2\}$, we compute the error on the test set as the average of the errors on the test set.
  $$\mathbb{E}[\mathcal{L}_{\text {test}}^{i}] = \frac{1}{T} \sum^T_{t=1} \mathcal{L}_{\text {test}, t}^{i}$$


\end{enumerate}
\subsection*{Question T2: Gradient of Multi-Class Logistic Regression (10 points)}
The loss function on a single sample $(x, y)$ for a logistic regression model with parameters $w$ for the multi-class classification problem can be written as
$$
\mathcal{L}_{w}(x, y)=-\sum_{j=1}^{K} y_{j} \cdot \log p_{j},
$$
where $K$ is the number of classes, $y_{j}$ is the ground truth label corresponding to the $j$-th class for the current sample, and $p_{j}$ is defined as:
$$
\begin{aligned}
p_{j} &=\sigma\left(w^{T} \cdot x\right)_{j} \\
&=\frac{e^{w_{j}^{T} \cdot x}}{\sum_{j=1}^{K} e^{w_{j}^{T} \cdot x}}
\end{aligned}
$$
The function $\sigma()$ is also called the Softmax and the loss function $\mathcal{L}_{w}$ is called the cross-entropy loss: by far the most popular loss function used to solve multiclass classification tasks.

Compute the gradient of the above loss function with respect to the parameter vector $w$. Show all the steps of the derivation.

\paragraph{Answer:} First compute the gradient of the Softmax function $p_j$ with respect to the parameter $w_i$ and $w_j$ where $i\neq j$:
\begin{align*}
  \pdv{p_j}{w_j} &=\pdv{}{w_j}\frac{e^{w_{j}^{T} \cdot x}}{\sum_{j=1}^{K} e^{w_{j}^{T} \cdot x}}\\
  &=\frac{\pdv{e^{w_{j}^{T} \cdot x}}{w_j}\cdot \sum_{j=1}^{K} e^{w_{j}^{T} \cdot x}-e^{w_{j}^{T} \cdot x} \cdot \sum_{j=1}^{K} \pdv{e^{w_{j}^{T} \cdot x}}{w_j}}{[\sum_{j=1}^{K} e^{w_{j}^{T} \cdot x}]^2}\\
  &=\frac{x e^{w_{j}^{T} \cdot x } \cdot \sum_{j=1}^{K} e^{w_{j}^{T} \cdot x}-e^{w_{j}^{T} \cdot x} \cdot \sum_{j=1}^{K} x e^{w_{j}^{T} \cdot x }}{[\sum_{j=1}^{K} e^{w_{j}^{T} \cdot x}]^2}\\
  &=x\frac{e^{w_{j}^{T} \cdot x }}{\sum_{j=1}^{K} e^{w_{j}^{T} \cdot x}}- x \frac{e^{w_{j}^{T} \cdot x} \cdot \sum_{j=1}^{K} e^{w_{j}^{T} \cdot x }}{[\sum_{j=1}^{K} e^{w_{j}^{T} \cdot x}]^2}\\
  &=x p_{j}- x p^2_{j}\\
\end{align*}
\begin{align*}
  \pdv{p_j}{w_i} &=\pdv{}{w_i}\frac{e^{w_{j}^{T} \cdot x}}{\sum_{j=1}^{K} e^{w_{j}^{T} \cdot x}}\\
  &=e^{w_{j}^{T} \cdot x} \pdv{\left(\sum_{j=1}^{K} e^{w_{j}^{T} \cdot x}\right)^{-1}}{w_i}\\
  &=- x e^{w_{j}^{T} \cdot x} e^{w_{i}^{T} \cdot x} \left(\sum_{j=1}^{K} e^{w_{j}^{T} \cdot x}\right)\left(\sum_{i\neq j}^{K} e^{w_{i}^{T} \cdot x}\right)\\
  &=-x p_j p_i
\end{align*}

The gradient of the loss function with respect to $w$ is given by:

\begin{align*}
  \grad_{w}{\mathcal{L}_{w}(x,y)} &=-\pdv{}{w_j}\sum_{i=1}^{K} y_{i} \cdot \log p_{i} \\
  &=-\sum_{i=1, i\neq j}^{K} y_{i} \pdv{\log p_{i}}{w_j} - y_j \pdv{\log p_{j}}{w_j}  \\
  &=-\sum_{i=1, i\neq j}^{K} y_{i} p_i^{-1} \pdv{p_{i}}{w_j} - y_j p_j^{-1} \pdv{p_{j}}{w_j}  \\
  &=\sum_{i=1, i\neq j}^{K} y_{i} p_i^{-1} x p_j p_i - y_j p_j^{-1} (x p_{j}- x p^2_{j})  \\
  &=\sum_{i=1, i\neq j}^{K} x y_{i} p_j + x y_j p_{j} - x y_j   \\
  &=x p_j \sum_{i=1}^{K} y_{i} - x y_j   \\
  &= x p_j - x y_j
  % &= -\sum_{j=1}^{K} y_{j} \cdot \pdv{}{w} \log \sigma\left(w^{T} \cdot x\right)_{j} \\
  % &= -\sum_{j=1}^{K} y_{j} \cdot  \frac{\pdv{\sigma\left(w^{T} \cdot x\right)_{j}}{w}}{\sigma\left(w^{T} \cdot x\right)_{j}}{w} \\
  % &= -\sum_{j=1}^{K} \pdv{}{w} y_{j} \cdot \log{\frac{e^{w_{j}^{T} \cdot x}}{\sum_{j=1}^{K} e^{w_{j}^{T} \cdot x}}}\\
  % &= -\sum_{j=1}^{K} \pdv{}{w}[y_{j} \cdot \log e^{w_{j}^{T} \cdot x}  - y_{j} \cdot \log \sum_{j=1}^{K} e^{w_{j}^{T} \cdot x}]\\
  % &= -\sum_{j=1}^{K} \pdv{}{w}[y_{j} (w_{j}^{T} \cdot x) - y_{j} \cdot \log \sum_{j=1}^{K} e^{w_{j}^{T} \cdot x}]\\
  % &= -\sum_{j=1}^{K} w^T_{j} \cdot y_j \cdot \log e^{w_{j}^{T} \cdot x}  - \pdv{}{w} y_{j} \cdot \log \sum_{j=1}^{K} e^{w_{j}^{T} \cdot x}
  \end{align*}

\subsection*{Question T3: Maximum Likelihood Estimate of a Gaussian Model (10 Points)}
Assume you are given a dataset $\mathcal{D}$ of $n$ real numbers $\mathcal{D}=\left\{x_{1}, x_{2}, \ldots, x_{n}\right\}$, where $x_{i} \in \Re, \forall i$. Derive the maximum likelihood estimate of the mean $\mu$ and variance $\sigma$, of the 1 -dimensional Gaussian distribution. Note that $\mu$ and $\sigma$ are the learnable parameters.

\begin{enumerate}
  \item Write down the expression of the $\log$-likelihood $\mathcal{L}_{\mu, \sigma}(\mathcal{D})$ of the data set $\mathcal{D}$ as a function of $\mu$ and $\sigma$. (2 points)

  \item Compute the partial derivative of $\mathcal{L}_{\mu, \sigma}(\mathcal{D})$ with respect to $\mu$, equate to zero and solve for $\mu$. (4 points)

  \item Compute the partial derivative of $\mathcal{L}_{\mu, \sigma}(\mathcal{D})$ with respect to $\sigma$, equate to zero and solve for $\sigma$. (4 points)

\end{enumerate}
\subsection*{Question T4: Hinge loss gradients (5 points)}
Unlike the Cross-Entropy loss, the Hinge loss (defined below), is not differentiable everywhere with respect to the parameters $\theta$ :
$$
\mathcal{L}_{\text {Hinge }}(x, y, \theta)=\max \left[0,1-y \cdot f_{\theta}(x)\right],
$$
for some parametric function $f_{\theta}$. Does it mean that we cannot use a gradientbased optimization algorithm for finding a solution that minimizes the hinge loss? If not, what can we do about it?

\section*{Practicum}
See the accompanying Python notebook.

Question P1: Metrics for a binary classifier ( 20 points)

Question P2: Support Vector Machines (50 points)


\end{document}