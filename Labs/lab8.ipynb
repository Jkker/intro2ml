{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84f30d75",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning (CSCI-UA.473)\n",
    "\n",
    "## Lab 1: Decision Trees and Random Forests\n",
    "### Date: November 11th, 2021\n",
    "\n",
    "\n",
    "### Name: (your name goes here)\n",
    "### Email: (your NYU email goes here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec244cd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(criterion='entropy')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 450x450 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Lets first create a basic decision tree for a very basic dataset!\n",
    "from sklearn import tree, metrics\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "BLUE, ORANGE, GRAY = '#57B5E8', '#E69E00', '#646369'\n",
    "\n",
    "#Our training data X is composed of two features, and our predictions are binary (0 or 1)\n",
    "X = np.array([[0, 0], [1, 1], [2,2], [2,3], [0,2], [1,2]])\n",
    "Y = np.array([0, 1, 0, 1, 0, 0 ])\n",
    "\n",
    "def plot_data(X, Y):\n",
    "    fig, ax = plt.subplots(figsize=(3, 3), dpi=150)\n",
    "    ax.set_aspect(1.3)\n",
    "    ax.scatter(X[:, 0], X[:, 1], s=18, facecolors='none',\n",
    "               edgecolors=np.array([BLUE, ORANGE])[Y])\n",
    "    ax.tick_params(\n",
    "        bottom=True, left=True, labelleft=False, labelbottom=False)\n",
    "    ax.text(0, 3.2, 'Training Data', color=GRAY, fontsize=9)\n",
    "    return fig, ax\n",
    "\n",
    "_, _ = plot_data(X, Y)\n",
    "\n",
    "# Default splitting is done using Gini impurity, we will use entropy for our exercise.\n",
    "clf = tree.DecisionTreeClassifier(criterion=\"entropy\") \n",
    "clf.fit(X,Y)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d70fa5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,\n",
       "       0, 0, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X = 3 + 2.5 * np.random.randn(50, 2)\n",
    "clf.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "191da7cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can also see the probability with which the classifier assigns each class for every sample\n",
    "clf.predict_proba(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5810ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The decision tree seems to be pretty confident on all of its predictions. Any ideas how we can confuse it?\n",
    "# HINT : Here probabilty can be thought of as the number of samples in the leaf node with the same class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2301525",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'graphviz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3294/3313789872.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# This was just a programmatic dataset, lets try the decision tree on a more meaningful one\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_digits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgraphviz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# We use the digits dataset provided by sklearn, this is similar to MNIST but much coarser (8x8) and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'graphviz'"
     ]
    }
   ],
   "source": [
    "# This was just a programmatic dataset, lets try the decision tree on a more meaningful one\n",
    "from sklearn.datasets import load_digits\n",
    "import graphviz\n",
    "\n",
    "# We use the digits dataset provided by sklearn, this is similar to MNIST but much coarser (8x8) and \n",
    "# thus a lot lighter than MNIST (28x28)\n",
    "dataset = load_digits()\n",
    "X, Y = dataset.data, dataset.target\n",
    "idxs = np.arange(0, len(X))\n",
    "np.random.shuffle(idxs)\n",
    "train_idxs,test_idxs = idxs[:1500], idxs[1500:]\n",
    "train_X, train_Y = X[train_idxs], Y[train_idxs]\n",
    "test_X, test_Y = X[test_idxs], Y[test_idxs]\n",
    "plt.gray()\n",
    "plt.matshow(dataset.images[1000])\n",
    "print(dataset.target[1000])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbe52e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = tree.DecisionTreeClassifier(criterion='entropy')\n",
    "clf = clf.fit(train_X, train_Y)\n",
    "dot_data = tree.export_graphviz(clf, out_file='graph.dot', \n",
    "                      feature_names=dataset.feature_names,  \n",
    "                      class_names=str(dataset.target_names),  \n",
    "                      filled=True, rounded=True,  \n",
    "                      special_characters=True) \n",
    "graph = graphviz.Source(dot_data)\n",
    "!dot -Tpng graph.dot -o graph.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f52b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict using the fitted decision tree\n",
    "preds = clf.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8b8286",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(preds == test_Y)/len(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27233c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This was your vanilla Decision trees, now lets look at Bagging.\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "# Remember, Bagging is just using an ensemble of decision trees to add more variance to your model.\n",
    "# So we simply wrap our original DecisionTreeClassifier with a BaggingClassifier module.\n",
    "clf = BaggingClassifier(base_estimator=tree.DecisionTreeClassifier(criterion='entropy'),\n",
    "                       n_estimators=1, max_samples=1.0, max_features=1.0, bootstrap=True)\n",
    "clf = clf.fit(train_X, train_Y)\n",
    "# What would setting n_estimators as 1 mean?\n",
    "# Changing the value for n_estimators changes our accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295bcd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = clf.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491d39b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(preds == test_Y)/len(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ad791e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forests\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Now we try RandomForests on the same data. Again, RandomForests is just a method to ensemble your base models\n",
    "# and will be used in the same way bagging was.\n",
    "clf = RandomForestClassifier(n_estimators=5)\n",
    "clf.fit(train_X, train_Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550a607d",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = clf.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db28b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(preds == test_Y)/len(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1aad7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we implement gradient boosting, in particular the Adaboost algorithm.\n",
    "# Remember, gradient boosting algorithms involve iteratively improving the decision trees\n",
    "# and hence involve a learning rate similar to logistic regressions.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.datasets import make_gaussian_quantiles\n",
    "\n",
    "# Construct dataset\n",
    "X1, y1 = make_gaussian_quantiles(\n",
    "    cov=2.0, n_samples=200, n_features=2, n_classes=2, random_state=1\n",
    ")\n",
    "X2, y2 = make_gaussian_quantiles(\n",
    "    mean=(3, 3), cov=1.5, n_samples=300, n_features=2, n_classes=2, random_state=1\n",
    ")\n",
    "X = np.concatenate((X1, X2))\n",
    "y = np.concatenate((y1, -y2 + 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4699602e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training points\n",
    "plot_colors = \"br\"\n",
    "plot_step = 0.02\n",
    "class_names = \"AB\"\n",
    "for i, n, c in zip(range(2), class_names, plot_colors):\n",
    "    idx = np.where(y == i)\n",
    "    plt.scatter(\n",
    "        X[idx, 0],\n",
    "        X[idx, 1],\n",
    "        c=c,\n",
    "        cmap=plt.cm.Paired,\n",
    "        s=20,\n",
    "        edgecolor=\"k\",\n",
    "        label=\"Class %s\" % n,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3173502e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and fit an AdaBoosted decision tree\n",
    "bdt = AdaBoostClassifier(\n",
    "    tree.DecisionTreeClassifier(max_depth=1, criterion='entropy'),algorithm=\"SAMME\", n_estimators=20, learning_rate=0.01\n",
    ")\n",
    "bdt.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb88d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the decision boundaries\n",
    "plt.subplot(111)\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(\n",
    "    np.arange(x_min, x_max, plot_step), np.arange(y_min, y_max, plot_step)\n",
    ")\n",
    "\n",
    "Z = bdt.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "cs = plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)\n",
    "plt.axis(\"tight\")\n",
    "\n",
    "# Plot the training points\n",
    "for i, n, c in zip(range(2), class_names, plot_colors):\n",
    "    idx = np.where(y == i)\n",
    "    plt.scatter(\n",
    "        X[idx, 0],\n",
    "        X[idx, 1],\n",
    "        c=c,\n",
    "        cmap=plt.cm.Paired,\n",
    "        s=20,\n",
    "        edgecolor=\"k\",\n",
    "        label=\"Class %s\" % n,\n",
    "    )\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max)\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"Decision Boundary\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94691abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the two-class decision scores\n",
    "twoclass_output = bdt.decision_function(X)\n",
    "plot_range = (twoclass_output.min(), twoclass_output.max())\n",
    "plt.subplot(122)\n",
    "for i, n, c in zip(range(2), class_names, plot_colors):\n",
    "    plt.hist(\n",
    "        twoclass_output[y == i],\n",
    "        bins=10,\n",
    "        range=plot_range,\n",
    "        facecolor=c,\n",
    "        label=\"Class %s\" % n,\n",
    "        alpha=0.5,\n",
    "        edgecolor=\"k\",\n",
    "    )\n",
    "x1, x2, y1, y2 = plt.axis()\n",
    "plt.axis((x1, x2, y1, y2 * 1.2))\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.ylabel(\"Samples\")\n",
    "plt.xlabel(\"Score\")\n",
    "plt.title(\"Decision Scores\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(wspace=0.35)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
