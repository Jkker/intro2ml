{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning (CSCI-UA.473)\n",
    "## Lab 9: Principal Component Analysis and Non-Negative Matrix Factorization\n",
    "### Date: November 18th, 2021\n",
    "### Name: (your name goes here)\n",
    "### Email: (your NYU email goes here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction\n",
    "\n",
    "We are going to learn about implementing a couple of dimensionality reduction techniques and visualizing the results. In particular we will cover: \n",
    "\n",
    "1. PCA = Principle Component Analysis\n",
    "2. NMF = Non-negative Matrix Factorization\n",
    "\n",
    "As discussed in the lecture the two techniques differ in the constraints they impose on the dictionary elements and the representations, which in-turn leads to a completely different objective function and the optimization algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis\n",
    "We will start with the PCA, which involves computing the projections of the original input so that the reconstruction error is minimized.\n",
    "\n",
    "$$\n",
    "\\arg \\min_{W,Z} \\| X - WZ \\|^2_F\n",
    "$$\n",
    "\n",
    "This is equivalent to maximizing the variance of the projected data and can be efficiently computed by decomposing the data matrix $X$ into its singular values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset\n",
    "\n",
    "We will first load the standard packages and then load and use the b/w faces data set, which is avaialable to download from here: \n",
    "\n",
    "https://github.com/HyTruongSon/Pattern-Classification/tree/master/MIT-CBCL-database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the standard packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "from scipy import linalg\n",
    "from scipy.sparse import diags\n",
    "import scipy.optimize\n",
    "\n",
    "import autograd\n",
    "from autograd import grad\n",
    "import autograd.numpy as numpy\n",
    "import autograd.numpy.random as npr\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# the normalize function which first computes the mean \n",
    "# and then subtracts it from all the data points\n",
    "def normalize(data):\n",
    "    mean = np.mean(data, axis=1)[:,None]\n",
    "    return data - mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset and display a test image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = open('./svm.train.normgrey').readlines()\n",
    "X = []\n",
    "\n",
    "for line in raw_data[2:]:\n",
    "    if line.strip() == \"\":\n",
    "        continue\n",
    "    if int(line.split()[-1]) == -1:\n",
    "        # not face, skip\n",
    "        continue\n",
    "    else:\n",
    "        X.append( [float(yy) for yy in line.split()[:-1]] )\n",
    "\n",
    "X = np.array(X)\n",
    "print('Input data shape: {}'.format(X.shape))\n",
    "\n",
    "plt.imshow(X[0].reshape((19, 19)), cmap=cm.gray, interpolation=\"bilinear\")\n",
    "\n",
    "X = normalize(X)\n",
    "X = np.transpose(X)\n",
    "# Why transpose?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the size of the original input space and also set the size of the new low-dimensional output space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_comp = 121  # how many dimensions will be in the new space\n",
    "\n",
    "(d, N) = X.shape\n",
    "q = n_comp\n",
    "\n",
    "print (\"d: old dim\",  d)\n",
    "print (\"N: num of samples\", N)\n",
    "print (\"q: new dim\", q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the Principal Components\n",
    "\n",
    "We use the Singular Value Decomposition of the matrix X to compute all the components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pca_svd(input_data):\n",
    "    U, S, V = linalg.svd(X)  # SVD decomposition\n",
    "    print('Input dimensions: {}'.format(input_data.shape))\n",
    "    print('Left singular vector matrix shape: {}'.format(U.shape))\n",
    "    print('Right singular vector matrix shape: {}'.format(V.shape))\n",
    "    print('Singular value matrix shape: {}'.format(S.shape))\n",
    "    \n",
    "    return S, U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_faces, U_faces = train_pca_svd(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to compute the principal components, we take the most impactful columns of the left singular matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take q most impactful columns from U. These are your principal components\n",
    "top_vectors = U_faces[:, :q]\n",
    "# reduce dimensionality of our input data by projecting each point onto the principal components\n",
    "X_reduced = X.transpose() @ top_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets check the shape of the old and the new data matrix\n",
    "np.transpose(X).shape, X_reduced.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of principle components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadings = top_vectors.transpose()\n",
    "print(loadings.shape)\n",
    "f, axarr = plt.subplots(int(np.sqrt(n_comp)), int(np.sqrt(n_comp)), figsize = (15, 15))\n",
    "\n",
    "comp_ix = 0\n",
    "# loop over the principal components and display them as an image of size 19x19\n",
    "for j in range(int(np.sqrt(n_comp))):\n",
    "    for i in range(int(np.sqrt(n_comp))):\n",
    "        axarr[i, j].imshow(loadings[comp_ix].reshape((19, 19)), cmap=cm.gray, interpolation=\"bilinear\")\n",
    "        comp_ix += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the reconstruction error between the original and reconstructed image\n",
    "reconstruction_err = np.linalg.norm( X - top_vectors @ np.transpose(X_reduced) )\n",
    "print ('Reconstructions error: ', reconstruction_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconstruct the image back from the reduced space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_face(idx):\n",
    "    original_face = X[:,idx]\n",
    "    reduced_face = X_reduced[idx]\n",
    "    reconstructed_face = reduced_face @ loadings\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(original_face.reshape((19, 19)), cmap=cm.gray, interpolation=\"bilinear\")\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(reconstructed_face.reshape((19, 19)), cmap=cm.gray, interpolation=\"bilinear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the reconstructed face does not look that bad\n",
    "reconstruct_face(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Negative Matrix Factorization (NMF)\n",
    "The optimization problem for non-negative matrix factorization is similar to the PCA, except it has additional non-negativity constraints on $W$ and $Z$. \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\arg \\min_{W,Z} & \\| X - WZ \\|^2_F \\\\\n",
    "s.t. \\quad & w_{ij} \\ge 0 \\quad i = 1, \\ldots, n \\quad j = 1, \\ldots, p \\\\\n",
    "           & z_{ij} \\ge 0 \\quad i = 1, \\ldots, p \\quad j = 1, \\ldots, n \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Presence of additional constraints makes the optimization problem a little harder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nmf(input_data):\n",
    "    multiplicative = True\n",
    "    maxiter = 1000\n",
    "    X = input_data\n",
    "    W = np.real( (np.random.rand(d,q))*0.5 )\n",
    "    Z = np.real( (np.random.rand(q,N))*0.5 )\n",
    "    print ('start optimising...')\n",
    "    # X : (d, N)\n",
    "    # W : (d, q)\n",
    "    # Z : (q, N)        \n",
    "    for u_iter in range(maxiter):\n",
    "        if multiplicative:\n",
    "            diff = X - W @ Z\n",
    "            W = W * (X @ np.transpose(Z)) / (W @ Z @ np.transpose(Z))\n",
    "            Z = Z * (np.transpose(W) @ X) / (np.transpose(W) @ W @ Z)\n",
    "        else:\n",
    "            diff = X - W @ Z\n",
    "            lr = 0.001\n",
    "            W_ = W + lr * (diff) @ np.transpose(Z)\n",
    "            Z_ = Z + lr * np.transpose(W) @ (diff)       \n",
    "            W, Z = np.maximum(W_, 0, W_), np.maximum(Z_, 0, Z_)\n",
    "\n",
    "        if u_iter % 100 == 0:\n",
    "            print (autograd.numpy.linalg.norm(diff))\n",
    "        \n",
    "    print ('optimization finished')\n",
    "    return W, Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X - np.min(X)  # why do we do that?\n",
    "\n",
    "loadings, X_reduced = train_nmf(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadings = loadings.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the factorization learnt\n",
    "f, axarr = plt.subplots(int(np.sqrt(n_comp)), int(np.sqrt(n_comp)), figsize = (15, 15))\n",
    "\n",
    "comp_ix = 0\n",
    "for j in range(int(np.sqrt(n_comp))):\n",
    "    for i in range(int(np.sqrt(n_comp))):\n",
    "        axarr[i, j].imshow(loadings[comp_ix].reshape((19, 19)), cmap=cm.gray, interpolation=\"bilinear\")\n",
    "        comp_ix += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Featurizing text data and reducing its dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data first. Each document will be a training sample and it will be represented as a tf-idf vector. Tf-idf which is an abbreviation of **Term Frequency Inverse Document Frequency** and its definition is provided here: https://en.wikipedia.org/wiki/Tf%E2%80%93idf  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "newsgroups_train = datasets.fetch_20newsgroups(subset='train', \n",
    "                                               categories=['comp.sys.mac.hardware', 'rec.motorcycles', 'sci.med', 'soc.religion.christian'])\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=30)\n",
    "X = tfidf_vectorizer.fit_transform(newsgroups_train.data).toarray()\n",
    "print(len(tfidf_vectorizer.get_feature_names()))\n",
    "y = newsgroups_train.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.transpose(X)\n",
    "\n",
    "(d, N) = X.shape\n",
    "q = 5  # our reduced dimension\n",
    "\n",
    "print (\"d: old dim\",  d)\n",
    "print (\"N: num of samples\", N)\n",
    "print (\"q: new dim\", q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_text, V_text = train_pca_svd(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take q most impactful columns from V\n",
    "top_vectors = V_text[:, :q]\n",
    "\n",
    "# reduce dimensionality of our input data\n",
    "X_reduced = X.transpose() @ top_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_reduced[:,0], X_reduced[:, 1], c = y)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadings = top_vectors.transpose()\n",
    "invocab = dict([(vv,kk) for kk, vv in tfidf_vectorizer.vocabulary_.items()])\n",
    "insens_idx = np.argsort(loadings[0])\n",
    "\n",
    "print ('Top 10 most activated words (loadings)')\n",
    "for jj in insens_idx[-10:]:\n",
    "    print ('{} ({:0.6f})'.format(invocab[jj], loadings[0,jj]))\n",
    "           \n",
    "    \n",
    "print ('\\n Top 10 least activated words (loadings)')\n",
    "for jj in insens_idx[:10]:\n",
    "    print ('{} ({:0.6f})'.format(invocab[jj], loadings[0,jj]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruction_err = np.linalg.norm( X - top_vectors @ np.transpose(X_reduced) )\n",
    "print ('Reconstructions error: ', reconstruction_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What exactly do the principal components represent? \n",
    "What kind of features are being learnt? Lets see. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invocab = dict([(vv,kk) for kk, vv in tfidf_vectorizer.vocabulary_.items()])\n",
    "\n",
    "for i in range(q):\n",
    "    insens_idx = np.argsort(loadings[i])\n",
    "\n",
    "    print ('COMPONENT ' + str(i))\n",
    "    print (' '.join([invocab[w_ix] for w_ix in insens_idx[-10:]]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
