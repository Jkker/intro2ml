{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning (CSCI-UA.473)\n",
    "\n",
    "## Homework 1: Linear Models for Regression and Classification\n",
    "### Due: September 27th, 2021 at 11:59PM\n",
    "\n",
    "\n",
    "### Name: (your name goes here)\n",
    "### Email: (your NYU email goes here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question P1: Linear Regression (20 points total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A linear regression model is a linear function of the parameters of the model. Its output $y$ is a real number or a real vector, depending on the task definition. That is: \n",
    "$$\n",
    "y = w \\cdot x\n",
    "$$\n",
    "\n",
    "Implement, train, and test a linear regression model to predict the values of PSA. \n",
    "\n",
    "The dataset can be found here: https://web.stanford.edu/~hastie/ElemStatLearn/datasets/prostate.data \n",
    "The necessary information associated with the dataset can be found here: https://web.stanford.edu/~hastie/ElemStatLearn/datasets/prostate.info.txt\n",
    "\n",
    "Note that the last column indicates which observations belong to the training set and which ones belong to the test set. For the purpose of this exercise use the test set as your \"validation\" set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P1.a: Loading the data, implementing the model, and finding the solution (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset ###\n",
    "Load the dataset and split it into train and validation sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load the dataset here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement the model, find the solution ###\n",
    "1. Find the closed form solution and report the numbers on the validation set. \n",
    "2. Find another solution using LinearRegression module of scikit-learn and report any difference in numbers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your model implementation and the two solutions go here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P1.b: Implement Ridge and Lasso regression (5 + 5 = 10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ridge Regression\n",
    "Implement the Ridge regression and plot the values of coefficients for different values of the effective degrees of freedom $df(\\lambda)$: \n",
    "\n",
    "$$\n",
    "df(\\lambda) = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9].\n",
    "$$\n",
    "\n",
    "Where the effective degrees of freedom $df(\\lambda)$ is defined as: \n",
    "\n",
    "$$ \n",
    "df(\\lambda) = trace \\left[{\\bf X}({\\bf X}^T {\\bf X} +  \\lambda {\\bf I})^{-1} {\\bf X}^T \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your ridge regression code and the plotting code goes here ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lasso Regression\n",
    "Implement the Lasso regression and plot the values of coefficients for different values of the shrinkage parameters $s$: \n",
    "\n",
    "$$\n",
    "s = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0].\n",
    "$$\n",
    "\n",
    "Where the shrinkage parameters $s$ is defined as: \n",
    "\n",
    "$$ \n",
    "s = \\frac{t}{\\sum_{i=1}^p |\\beta_i|}, \n",
    "$$\n",
    "\n",
    "and $t$ is the sparsity penality in the Lasso equation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your lasso regression code and the plotting code goes here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question P2: Gradient Descent (10 points total)\n",
    "\n",
    "In this problem you will implement gradient descent as a general purpose optimization algorithm.\n",
    "\n",
    "You will need the following packages below to do the homework. Please DO NOT import any other packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd import grad\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import multivariate_normal as mvn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P2.a: Implement gradient descent with a fixed learning rate (5 points)\n",
    "\n",
    "Using autograd to compute the derivative, implement gradient descent with a fixed learning rate `lr` for a general scalar function `fun`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Perform a fixed number of iterations of gradient descent on a function using a fixed learning rate. \n",
    "\n",
    "Input:\n",
    "    fun : function handle: function that takes in a numpy array of shape (d,) and returns a float\n",
    "    x0  : initial point: numpy array of shape (d,)\n",
    "    lr  : fixed learning rate: a positive float\n",
    "    iterations : number of iterations to perform: int\n",
    "    \n",
    "Return:\n",
    "    x   : minimizer to fun: numpy array of shape (d,)\n",
    "\"\"\"\n",
    "def gd_fixed(fun, x0, lr, iterations):\n",
    "    \n",
    "    # TO DO:\n",
    "        \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P2.b: Implement gradient descent with a variable learning rate ( 5 points)\n",
    "\n",
    "Sometimes it is necessary to decrease our learning rate as we iterate to help gradient descent converge. Implement gradient descent below where the learning rate at iteration $i$ is given by\n",
    "\n",
    "$$\n",
    "\\mathrm{lr}_i = \\frac{\\mathrm{lr}}{i+1}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Perform a fixed number of iterations of gradient descent on a function using a variable learning rate. \n",
    "\n",
    "Input:\n",
    "    fun : function handle: function that takes in a numpy array of shape (d,) and returns a float\n",
    "    x0  : initial point: numpy array of shape (d,)\n",
    "    lr  : initial learning rate: a positive float\n",
    "    iterations : number of iterations to perform: int\n",
    "    \n",
    "Return:\n",
    "    x   : minimizer to fun: numpy array of shape (d,)\n",
    "\"\"\"\n",
    "def gd_variable(fun, x0, lr, iterations):\n",
    "    \n",
    "    # TO DO:\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question P3: Logistic Regression (30 points total)\n",
    "\n",
    "You will now implement the logistic regression from scratch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P3.a: Implement the logisitc unit (5 points)\n",
    "\n",
    "For binary classification with label $y \\in \\{0,1\\}$ we can model the posterior probability $p(y = 1 | x)$ with the logistic unit \n",
    "\n",
    "$$\n",
    "h(x; w) = \\frac{1}{1 + \\exp(-w^Tx)} .\n",
    "$$\n",
    "\n",
    "We will use the convention that $x_0 = 1$.  Implement this function below using the skeletal outline as a guide.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-19-5cd7a07f99fd>, line 14)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-19-5cd7a07f99fd>\"\u001b[0;36m, line \u001b[0;32m14\u001b[0m\n\u001b[0;31m    h = ?\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Evaluate the logistic unit h(x; w) on each point x in the dataset X (each row is a different data point). This code \n",
    "should be vectorized for efficient computation (i.e., no for loops).\n",
    "\n",
    "Input:\n",
    "    w: weight vector: numpy array of shape (d,) where d is the dimension\n",
    "    X: data: numpy array of shape (n,d) where n is the number of data points\n",
    "\n",
    "Return:\n",
    "    logits : h(x;w): a numpy array of shape (n,) that has the values \n",
    "\"\"\"\n",
    "def logistic_unit(X, w):\n",
    "    # TO DO:\n",
    "    h = ?\n",
    "    return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P3.b: Implement the loss function (5 points)\n",
    "\n",
    "We have implicitly made the assumption that $y|x \\sim \\mathrm{Bernoulli}(h(x;w))$. If we have an iid dataset $\\{(x^i, y^i)\\}_{i=1}^N$, then we can write the likelihood of the data as: \n",
    "\n",
    "$$\n",
    "p(\\hat{y}| X, w) = \\prod_{i=1}^N p(y^i | x^i, w)\n",
    "$$\n",
    "\n",
    "We can learn the parameter $w$ by maximizing this probability (i.e. we find the maximum likelihood estimator) or equivalently minimizing the loss function $J(w) = -\\log p(\\hat{y} | X, w)$. Implement the loss function $J(w)$ using the skeletal outline below. In your implementation you may use the convention that $0 \\log 0 = 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Compute the loss function using the formula you derived.  This code does not need to be vectorized and you\n",
    "may use the logistic_unit function.\n",
    "\n",
    "Input:\n",
    "    w : weight vector: numpy array of shape (d+1,) (remember that x_0 = 1)\n",
    "    X : dataset features: numpy array of shape (n,d)\n",
    "    y : dataset targets: numpy array of shape (n,) contains only 0's or 1's\n",
    "\n",
    "Return:\n",
    "    J : loss of w given X,y: float\n",
    "\"\"\"\n",
    "def loss(w, X, y):\n",
    "    # TO DO:\n",
    "\n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making data (Nothing to do here)\n",
    "\n",
    "The following method generates random data to test your algorithm on.  **DO NOT CHANGE THE METHOD!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following method generates a random dataset.  DO NOT ALTER THIS METHOD.\n",
    "def make_data(n_samples = 500):\n",
    "    # Generate data features.\n",
    "    X1 = mvn.rvs(mean = np.array([1.1, 0]), cov = 0.2*np.eye(2), size = n_samples//2)\n",
    "    X2 = mvn.rvs(mean = np.array([-1.1, 0]), cov = 0.2*np.eye(2), size = n_samples - n_samples//2)\n",
    "    # Append data labels and combine.\n",
    "    X1 = np.hstack( (X1, np.ones((X1.shape[0], 1))))\n",
    "    X2 = np.hstack( (X2, np.zeros((X2.shape[0], 1))))\n",
    "    X = np.vstack([X1, X2])\n",
    "    # Randomly permute data.\n",
    "    np.random.shuffle(X)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P3.c: Splitting the data for training and testing (5 points)\n",
    "\n",
    "We split the dataset into a training, validation, and test set. Use a 40/40/20 split (roughly 40/40/20 is fine). You do not need to use a random splitting, although in practice it is usually a good idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first generate some fake data.\n",
    "np.random.seed(2) # Don't change the random seed.\n",
    "data = make_data()\n",
    "X = data[:,:-1] # Features\n",
    "y = data[:,-1]  # Labels\n",
    "\n",
    "# We'll also augment the data so that x_0 = 1 for the intercept term.\n",
    "X = np.append(np.ones((len(X), 1)), X, axis = 1)\n",
    "\n",
    "\n",
    "# TO DO:\n",
    "## Your part starts here.\n",
    "\n",
    "## Ends here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P3.d: Implement the model training (10 points)\n",
    "\n",
    "Now use your gradient descent function to learn the parameters $w$ using `5000` iterations. You may choose the learning rate and initial parameters $w_0$ for this problem. Compare both the fixed learning rate and variable learning rate gradient descents side by side (i.e., 2 subplots) by computing the loss after every `m=10` iterations. \n",
    "There are 4 things to do for this problem.\n",
    "\n",
    "1. Set the learning rate and initial points.\n",
    "2. Implement the loss function $J(w)$.\n",
    "3. Update the parameters using the two gradient descent methods.\n",
    "4. Compute the loss after every 10 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-23-9191419cca7c>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-23-9191419cca7c>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    lr = ?                # You choose this value, try something small.\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "iterations = 5000        # Lots of iterations to see the training error go down.\n",
    "m = 10                   # Get the loss every m iterations.\n",
    "\n",
    "# TO DO: Set the learning rate.\n",
    "lr = ?                # You choose this value, try something small.\n",
    "\n",
    "\n",
    "\n",
    "## Store the loss values in these arrays.\n",
    "# Fixed learning rate.\n",
    "train_loss_fixed = np.zeros(iterations//m)\n",
    "val_loss_fixed = np.zeros(iterations//m)\n",
    "\n",
    "# Variable learning rate.\n",
    "train_loss_var = np.zeros(iterations//m)\n",
    "val_loss_var = np.zeros(iterations//m)\n",
    "\n",
    "\n",
    "# TO DO: Set the initial values, either randomly or some fixed value.\n",
    "w1 = ?\n",
    "w2 = ?\n",
    "\n",
    "# TO DO: Write the loss function as a function of the parameter only.\n",
    "J = ?\n",
    "\n",
    "\n",
    "for i in range(iterations):\n",
    "    # TO DO: Get the updated parameters from gradient descent with a fixed learning rate.\n",
    "    w1 = ?\n",
    "    # TO DO: Get the updated parameters from gradient descent with a variable learning rate.\n",
    "    # Hint: You may either modify the gd_variable function above or call gd_fixed with a different learning rate.\n",
    "    w2 = ?\n",
    "    \n",
    "    # Only compute the loss every m iterations.\n",
    "    if np.mod(iterations, m) == 0: \n",
    "        # TO DO: Compute the training and validation loss of the parameters found with the fixed learning rate.\n",
    "        train_loss_fixed[i//m] = ?\n",
    "        val_loss_fixed[i//m] = ?\n",
    "        # TO DO: Compute the training and validation loss of the parameters found with a variable learning rate.\n",
    "        train_loss_var[i//m] = ?\n",
    "        val_loss_var[i//m] = ?\n",
    "\n",
    "        \n",
    "## Plotting starts here.  Nothing to implement.\n",
    "its = np.arange(1, iterations + 1, m)\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (10, 4))\n",
    "\n",
    "ax1.semilogy(its, train_loss_fixed, 'b-', label = 'Train')\n",
    "ax1.semilogy(its, val_loss_fixed, 'r-', label = 'Val')\n",
    "ax1.set_xlabel('Iteration')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('GD with Fixed Learning Rate')\n",
    "ax1.grid()\n",
    "ax1.legend()\n",
    "\n",
    "ax2.semilogy(its, train_loss_var, 'b-', label = 'Train')\n",
    "ax2.semilogy(its, val_loss_var, 'r-', label = 'Val')\n",
    "ax2.set_xlabel('Iteration')\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.set_title('GD with Variable Learning Rate')\n",
    "ax2.grid()\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P3.e: Evalute the model on the test set (5 points)\n",
    "\n",
    "Finally, we have trained our models and are ready to evaluate them on the test set. But first we need to define a discriminant function $f(x)$ which converts probabilities into discrete classes. It assigns the label $y = 1$ if $p(y = 1 | x) \\ge 0.5$ and $0$ otherwise. In other words, with a discriminant function we do not need the posterior probabilities but rather skip straight to the classification. Implement this discremenant function, infer the class for every point in the test set, and compute the following metric. \n",
    "\n",
    "For binary classification one way to check our classifier is to make a *confusion matrix* of our predictions.\n",
    "\n",
    "$$\n",
    "C = \\begin{bmatrix}\n",
    "\\text{Predict 0, Actual 0} & \\text{Predict 0, Actual 1}\\\\\n",
    "\\text{Predict 1, Actual 0} & \\text{Predict 1, Actual 1}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The diagonal elements are the number of samples that are correctly classified.\n",
    "\n",
    "Use both trained models (fixed and variable learning rates) to classify samples in the test set according to whether $h(x_i; w) \\ge 0.5$ or not and print the confusion matrices. Also print the accuracy rate which is just the percentage of correctly classified examples for both models.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C1 = np.zeros((2, 2)) # Confusion matrix for fixed learning rate.\n",
    "C2 = np.zeros((2, 2)) # Confusion matrix for variable learning rate.\n",
    "N = len(X_test)       # Number of test samples.\n",
    "\n",
    "\n",
    "## TO DO STARTS HERE:\n",
    "\n",
    "\n",
    "# Compute the accuracy         \n",
    "acc1 = ?\n",
    "acc2 = ?\n",
    "\n",
    "# TO DO ENDS HERE.\n",
    "print(\"C_1 = \")\n",
    "print(C1)\n",
    "print(\"C_2 = \")\n",
    "print(C2)\n",
    "print('Fixed Learning Rate Accuracy    = {:0.1f}%'.format(100 * acc1))\n",
    "print('Variable Learning Rate Accuracy = {:0.1f}%'.format(100 * acc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
