{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning (CSCI-UA.473)\n",
    "\n",
    "## Homework 3: Implementing kernelized support vector machines and backpropagation algorithm for a multi-layer perceptron\n",
    "\n",
    "### Due: November 3rd, 2021 at 11:59PM\n",
    "### Name: (your name goes here)\n",
    "### Email: (your NYU email goes here)\n",
    "\n",
    "**Please submit two files as part of your homework: the solved Python notebook and a pdf version of the final notebook. Create a zip file with these two files and name it as <netid>_hw3.zip**\n",
    "\n",
    "Please DO NOT change the position of any cell in this assignment. If the notebook hangs sometimes before the cell that defines SVM class, please restart it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need the following packages below to do the homework.  Please DO NOT import any other packages.\n",
    "### WARNING!\n",
    "Some parts below (especially with cross-validation) could take around ~10 - 15 min.  If it takes much longer than this, then you likely have an error. To keep track of the computation I would encourage you to think ways of inserting print statements at appropriate places in the code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from scipy.optimize import minimize\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and spliting the data\n",
    "For this assignment, we utilized the Iris dataset from sklearn library. We will use only the first two input variables of the data set as our input features. We will convert this into a binary classification task by ignoring class 0 and only working with classes 1 and 2. Lastly, we will split the data set into two sets, such that the training_set:test_set ratio is 70:30. \n",
    "* `X_train` and `y_train` are `features` and `labels` for training, while `X_test` and `y_test` are for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X = X[y != 0, :2] # Only use the first two features.\n",
    "y = y[y != 0]     # Ignore the first class.\n",
    "y[y==2] = -1      # Change class label to -1 for SVM.\n",
    "\n",
    "n_sample = len(X) # Total number of data points.\n",
    "\n",
    "# Split data into training and testing sets.\n",
    "#np.random.seed(0)\n",
    "order = np.random.permutation(n_sample)\n",
    "X = X[order]\n",
    "y = y[order].astype(np.float)\n",
    "\n",
    "X_train = X[:int(.7 * n_sample)]\n",
    "y_train = y[:int(.7 * n_sample)]\n",
    "X_test = X[int(.7 * n_sample):]\n",
    "y_test = y[int(.7 * n_sample):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question P1. Kernel SVM (50 Points Total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the assignment you will implement a kernelized version of the Support Vector Machines (SVMs). Below is a very brief overview of the kernel trick used in SVMs. \n",
    "\n",
    "Recall that for linear SVM the dual problem is given by \n",
    "\n",
    "\\begin{align*}\n",
    "\\max_{\\alpha} W(\\alpha) &= \\sum_{i=1}^n \\alpha_i - \\frac{1}{2}\\sum_{i=1}^{n}\\sum_{j=1}^{n} y_i y_j \\alpha_i \\alpha_j \\langle x_i, x_j\\rangle \\\\\n",
    "\\text{s.t.} & \\quad 0 \\le \\alpha_i \\le C, \\quad i = 1,\\ldots,n \\\\\n",
    "& \\sum_{i=1}^n \\alpha_i y_i = 0\n",
    "\\end{align*}\n",
    "\n",
    "where $\\alpha \\in \\mathbb{R}^n$ is a vector.  \n",
    "\n",
    "If the data is not linearly separable, we can project the input features $x_i$ into another potentially high-dimensional space denoted by $\\phi(x_i)$. The dual formulation of SVM in that space now becomes: \n",
    "\n",
    "\\begin{align*}\n",
    "\\max_{\\alpha} W(\\alpha) &= \\sum_{i=1}^n \\alpha_i - \\frac{1}{2}\\sum_{i=1}^{n}\\sum_{j=1}^{n} y_i y_j \\alpha_i \\alpha_j \\langle \\phi(x_i), \\phi(x_j)\\rangle \\\\\n",
    "\\text{s.t.} & \\quad 0 \\le \\alpha_i \\le C, \\quad i = 1,\\ldots,n \\\\\n",
    "& \\sum_{i=1}^n \\alpha_i y_i = 0\n",
    "\\end{align*}\n",
    "\n",
    "Since $\\phi(x_i)$ could potentially be very high dimensional, computing the dot product $\\langle \\phi(x_i), \\phi(x_j)\\rangle$ over the course of training and inference can be very expensive. In order to avoid the explicit computation of this dot product, we can make use of the ``kernel trick``, which says that the dot product in the high-dimensional space for certain feature maps $\\phi$ is equivalent to computing the value of the kernel in the original low-dimensional input space. That is: \n",
    "\n",
    "$$\n",
    "K(x_i, x_j) = \\langle \\phi(x_i), \\phi(x_j)\\rangle\n",
    "$$\n",
    "\n",
    "Thus the dual formulation of the SVM now becomes: \n",
    "\\begin{align*}\n",
    "\\max_{\\alpha} W(\\alpha) &= \\sum_{i=1}^n \\alpha_i - \\frac{1}{2}\\sum_{i=1}^{n}\\sum_{j=1}^{n} y_i y_j \\alpha_i \\alpha_j K(x_i, x_j) \\\\\n",
    "\\text{s.t.} & \\quad 0 \\le \\alpha_i \\le C, \\quad i = 1,\\ldots,n \\\\\n",
    "& \\sum_{i=1}^n \\alpha_i y_i = 0\n",
    "\\end{align*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P1.a: Implement the kernel functions (10 Points -- 2.5 Points Each)\n",
    "You will implement the following four kernels: \n",
    "\n",
    "1. `linear`: $k(x_i,x_j)=x_i^T x_j$ (this is effectively what you did in the previous homework)\n",
    "\n",
    "2. `poly`: $k(x_i,x_j)=\\left(x_i^T x_j + 1\\right)^2$\n",
    "\n",
    "3. `rbf`: $k(x_i,x_j) = \\exp\\left( -\\frac{1}{2}\\|x_i - x_j\\|^2 \\right)$\n",
    "\n",
    "4. `laplace`: $k(x_i,x_j)=\\exp\\left(-\\frac{1}{2}\\left\\| x_i - x_j \\right\\| \\right)$\n",
    "\n",
    "Complete the function below by implementing the kernel function for each case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_product(x1, x2, kernel = 'linear'):\n",
    "    \"\"\"\n",
    "    Compute the kernel product k(x1,x2) for different choices of the kernel k.\n",
    "    \n",
    "    Input:\n",
    "        x1: np.ndarray(p,), the first vector\n",
    "        x2: np.ndarray(p,), the second vector\n",
    "        kernel: str, a string which is the name of the kernel, must match one of the options below exactly\n",
    "        linear, poly, rbf, laplace\n",
    "    \n",
    "    Return:\n",
    "        k: float, the value of the kernel k(x1, x2)\n",
    "    \"\"\"\n",
    "    \n",
    "    ##TODO-start##\n",
    "    if kernel == 'linear':\n",
    "        k = ?\n",
    "        return k\n",
    "    elif kernel == 'poly':\n",
    "        k = ?\n",
    "        return k\n",
    "    elif kernel == 'rbf':\n",
    "        k = ?\n",
    "        return k\n",
    "    elif kernel == 'laplace':\n",
    "        k = ?\n",
    "        return k\n",
    "    ## TODO-end##\n",
    "    else:\n",
    "        print(\"Invalid kernel: {:s}\".format(kernel))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell is just a helper method. You do not need to implement anything in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_product_matrix(X, kernel = 'linear'):\n",
    "    \"\"\"\n",
    "    Compute the inner product matrix of two vectors.\n",
    "    \n",
    "    Input:\n",
    "        X: np.ndarray(n,p), data matrix with n data points (each row) and p features\n",
    "        kernel: str, a string which is the name of the kernel, must match one of the options below exactly\n",
    "        linear, poly, rbf, laplace\n",
    "    \n",
    "    Return:\n",
    "        K: np.ndarray(n,n), each entry is the kernel product of the corresponding pair of vectors\n",
    "    \"\"\"\n",
    "    n = len(X)\n",
    "    K = np.zeros((n, n))  \n",
    "    for i in range(n):\n",
    "        for j in range(i, n):\n",
    "            K[i, j] = kernel_product(X[i], X[j], kernel)\n",
    "            K[j, i] = K[i, j] # Matrix is symmetric so we can cut computation in half.          \n",
    "    return K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P1.b: Implement the kernel SVM (20 Points)\n",
    "\n",
    "Now you will extend your SVM implementation from the previous homework by including kernels.  Finish the SVM class below by filling in the missing lines of code. Similar to the previous homework, you will need to do 3 things: \n",
    "\n",
    "1. Implement the objective function for the dual problem for minimization.  Note that we actually maximize the dual objective function, but in order to use the `minimize()` function from Sci-Py you will need to take the negative.\n",
    "\n",
    "2. Compute the bias.  Use your implementation from the previous homework for guidance and think carefully about what needs to change.\n",
    "\n",
    "3. Implement the `predict()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM:\n",
    "    \n",
    "    \n",
    "    def __init__(self, C = 1, kernel = 'linear'):\n",
    "        \"\"\"\n",
    "        Initialize the SVM model.\n",
    "        \n",
    "        Input:\n",
    "            C: float, the regularization constant for SVM.\n",
    "            kernel: str, a string which is the name of the kernel, must match one of the options below exactly\n",
    "            linear, poly, rbf, laplace\n",
    "        \n",
    "        Return:\n",
    "            None\n",
    "        \"\"\"\n",
    "        assert C >= 0\n",
    "        self.C = C\n",
    "        self.kernel = kernel\n",
    "        # The following variables are set after fit() is called.\n",
    "        self.X_train = None\n",
    "        self.y_train = None\n",
    "        self.bias  = None\n",
    "        self.alpha = None\n",
    "     \n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "        Computes the parameters alpha and bias that determine the maximum-margin decision boundary for SVM.\n",
    "        bias will be a float, alpha is a np.ndarray(n,) vector of the dual variables.\n",
    "    \n",
    "        Input:\n",
    "            X_train: np.ndarray(n,p), matrix of training data features\n",
    "            y_train: np.ndarray(n, ), vector of training data labels\n",
    "        \n",
    "        Return:\n",
    "            None\n",
    "        \"\"\"\n",
    "        # Save the training data.\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        # Number of training points and dimension.\n",
    "        n, p = X_train.shape\n",
    "        \n",
    "        # Get the negative objective function to change maximization to minimization.\n",
    "        ##TODO-start##\n",
    "        W = ?\n",
    "        ##TODO-end##\n",
    "                                                     \n",
    "        # Initialization and constraints for optimization.\n",
    "        init_pt = np.zeros(n)\n",
    "        bnds = tuple([(0, self.C) for i in range(n)])\n",
    "        cons = ({'type': 'eq', 'fun': lambda x:  np.dot(x, y_train)})\n",
    "        # Solve the dual problem for SVM.\n",
    "        res = minimize(W, init_pt, method='SLSQP', bounds=bnds,\n",
    "               constraints=cons)   \n",
    "        alpha = res.x\n",
    "        self.alpha = alpha\n",
    "                                                     \n",
    "        # Compute the bias\n",
    "        ##TODO-start##\n",
    "        self.bias = ?\n",
    "        ##TODO-end##\n",
    "    \n",
    "                                                     \n",
    "    def predict(self, X_test):\n",
    "        \"\"\"\n",
    "        Compute the predictions y_pred on the test set using only the support vectors.\n",
    "    \n",
    "        Input:\n",
    "            X_test: np.ndarray(n,p), matrix of the test data\n",
    "    \n",
    "        Return:\n",
    "            y_pred: np.ndarray(n,), vector of the predicted labels, either +1 or -1\n",
    "        \"\"\"\n",
    "        ##TODO-start##\n",
    "        y_pred = ?\n",
    "        ##TODO-end##\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is nothing to do in the next cell. It is just a helper function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_pred, y_test):\n",
    "    \"\"\"\n",
    "    Computes the accuracy on the test set given the class predictions.\n",
    "    \n",
    "    Input:\n",
    "        y_pred: np.ndarray(n,), vector of predicted class labels\n",
    "        y_test: np.ndarray(n,), vector of true class labels\n",
    "    \n",
    "    Output:\n",
    "        float, accuracy of predictions\n",
    "    \"\"\"\n",
    "    return np.mean(y_pred*y_test > 0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draw the decision boundaries of all kernels (nothing to do here)\n",
    "The following is an illustration of decision boundaries for SVM with kernels. You do not need to do anything in the next cell.  It is only to check your work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_list = ['linear', 'rbf', 'poly', 'laplace']\n",
    "\n",
    "for kernel in kernel_list:\n",
    "    model = SVM(C=10, kernel=kernel)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    print(\"SVM with {:s} kernel, accuracy = {:0.2f}%\".format(kernel, 100*accuracy(y_pred, y_test)))\n",
    "\n",
    "    plt.figure()\n",
    "    plt.clf()\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, zorder=10, cmap=plt.cm.Paired,\n",
    "                edgecolor='k', s=20)\n",
    "\n",
    "    # Circle out the test data\n",
    "    plt.scatter(X_test[:, 0], X_test[:, 1], s=80, facecolors='none',\n",
    "                zorder=10, edgecolor='k')\n",
    "\n",
    "    plt.axis('tight')\n",
    "    x_min = X[:, 0].min()\n",
    "    x_max = X[:, 0].max()\n",
    "    y_min = X[:, 1].min()\n",
    "    y_max = X[:, 1].max()\n",
    "\n",
    "    XX, YY = np.mgrid[x_min:x_max:200j, y_min:y_max:200j]\n",
    "    XXYY = np.c_[XX.ravel(), YY.ravel()]\n",
    "    Z = model.predict(XXYY)\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(XX.shape)\n",
    "    plt.pcolormesh(XX, YY, Z > 0, cmap=plt.cm.Paired)\n",
    "    plt.title(kernel)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P1.c: Cross-validation to choose the regularization parameter $C$ (15 Points)\n",
    "\n",
    "Complete the cross-validation function below.  The data has already been randomly arranged for you.  There are 4 things you will need to do for each fold.\n",
    "\n",
    "1. Split the training data\n",
    "2. Train the model\n",
    "3. Get the predictions\n",
    "4. Compute the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(model, X_train, y_train, folds = 5):\n",
    "    \"\"\"\n",
    "    Perform k-fold cross-validation on model using the available training data.  You may assume that \n",
    "    the number of training data points is divisible by the number of folds.\n",
    "    \n",
    "    Input:\n",
    "        model: SVM object, an instance of the SVM class, must have fit and predict implemented.\n",
    "        X_train: np.ndarray(n,p), training data features\n",
    "        y_train: np.ndarray(n,), training data labels\n",
    "        folds: int, number of cross-validation folds to perform\n",
    "    \n",
    "    Output:\n",
    "        acc: float, the mean accuracy from all cross-validation folds\n",
    "        acc_results: np.ndarray(folds,), the accuracy results from each cross-validation fold\n",
    "    \"\"\"\n",
    "    n = X_train.shape[0] # Number of available training data points.\n",
    "    acc_results = np.zeros(folds) # Store the cross-validation results here.\n",
    "    \n",
    "    # Randomly permute the data.\n",
    "    permutation = np.random.permutation(n)\n",
    "    X_train = X_train[permutation, :]\n",
    "    y_train = y_train[permutation]\n",
    "    \n",
    "    for k in range(folds):\n",
    "        print(\"Fold {:d} / {:d} is running.\".format(k, folds))\n",
    "        \n",
    "        ##TODO-start##\n",
    "\n",
    "        ##TODO-end##\n",
    "    \n",
    "    acc = np.mean(acc_results)\n",
    "    return (acc, acc_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the next cell to check your work.  There is nothing you need to implement here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_list = [0.001, 0.01, 0.1, 0.5, 1, 5]\n",
    "\n",
    "for c in C_list:\n",
    "    print('Cross-validation for C = {:0.3f}'.format(c))\n",
    "    model = SVM(C = c, kernel = 'rbf')\n",
    "    acc, acc_results = cross_validation(model, X_train, y_train)\n",
    "    print('Mean = {:0.2f}%'.format(100*acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P1.d: Mean and standard deviation of cross-validation scores (5 Points)\n",
    "\n",
    "Now use 10-fold cross-validation on two models: 1. SVM with RBF kernel and 2. SVM with the linear kernel.  Set the regularization parameter $C = 0.5$ for both models.  Print out the mean accuracy and the standard deviation of the accuracies from each fold using the numpy functions `np.mean()` and `np.std()`.  Format your results as percentages to two decimal places using Python's `format()` function for strings so that it is easy to read.  Make sure the values you are printing are clearly indicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO-start##\n",
    "print('Cross-validation for SVM with RBF kernel')\n",
    "\n",
    "\n",
    "print('Cross-validation for SVM with linear kernel')\n",
    "\n",
    "##TODO-end##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question P2. Multi-layer perceptron training using backpropagation (40 Points Total)\n",
    "\n",
    "In this part of the homework you will implement a multi-layer perceptron model and train it using the backpropagation algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the packages. Please do not import any other packages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and splitting the data\n",
    "We will load and work with the half moons data set from sklearn and train a multi-layer perceptron to distinguish between the two classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of samples in the data set\n",
    "N_SAMPLES = 1000\n",
    "# ratio between training and test sets\n",
    "TEST_SIZE = 0.1\n",
    "\n",
    "# Double moon dataset\n",
    "X, y = make_moons(n_samples = N_SAMPLES, noise=0.2, random_state=100)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, random_state=42)\n",
    "\n",
    "def make_plot(X, y, plot_name, file_name=None, XX=None, YY=None, preds=None, dark=False):\n",
    "    plt.figure(figsize=(16,12))\n",
    "    axes = plt.gca()\n",
    "    axes.set(xlabel=\"$X_1$\", ylabel=\"$X_2$\")\n",
    "    plt.title(plot_name, fontsize=30)\n",
    "    plt.subplots_adjust(left=0.20)\n",
    "    plt.subplots_adjust(right=0.80)\n",
    "    if(XX is not None and YY is not None and preds is not None):\n",
    "        plt.contourf(XX, YY, preds.reshape(XX.shape), 25, alpha = 1, cmap=cm.Spectral)\n",
    "        plt.contour(XX, YY, preds.reshape(XX.shape), levels=[.5], cmap=\"Greys\", vmin=0, vmax=.6)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y.ravel(), s=40, cmap=plt.cm.Spectral, edgecolors='black')\n",
    "    if(file_name):\n",
    "        plt.savefig(file_name)\n",
    "        plt.close()\n",
    "        \n",
    "make_plot(X, y, \"Double Moon\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P2.a: Create and initialize the multi-layer perceptron (5 Points)\n",
    "In this section you will implement and initialize the weight layers of the multi-layer preceptron which has the following architecture: \n",
    "\n",
    "FC_2X25 -> ReLU_layer -> FC_25X50 -> ReLU_layer -> FC_50X25 -> ReLU_layer -> FC_25X1 -> Sigmoid_layer\n",
    "\n",
    "Where FC_InpXOut refers to the fully connected layer with `Inp` input units and `Out` output units. ReLU_layer and the Sigmoid_layer are the relu and sigmoid activation functions respectively. \n",
    "\n",
    "In the `init_layers` function, initialize all trainable parameters of the MLP model.\n",
    "\n",
    "`nn_architecture` is a list of dictionaries with layer specification.\n",
    "\n",
    "`seed` defines the random seed for all initial parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the architecture of the layers specified above\n",
    "NN_ARCHITECTURE = [\n",
    "    {\"input_dim\": 2, \"output_dim\": 25, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 25, \"output_dim\": 50, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 50, \"output_dim\": 25, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 25, \"output_dim\": 1, \"activation\": \"sigmoid\"},\n",
    "]\n",
    "\n",
    "\n",
    "# define the initialization function\n",
    "def init_layers(nn_architecture, seed = 42):\n",
    "    # random seed initiation\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # number of layers in our neural network\n",
    "    number_of_layers = ? \n",
    "    \n",
    "    # parameters storage initiation\n",
    "    params_values = {}\n",
    "    \n",
    "    # iteration over network layers\n",
    "    for idx, layer in enumerate(nn_architecture):\n",
    "        # we number network layers from 1\n",
    "        layer_idx = idx + 1\n",
    "        \n",
    "        # extracting the number of units in layers\n",
    "        layer_input_size = ? \n",
    "        layer_output_size = ? \n",
    "        \n",
    "        # initiating the values of the W matrix\n",
    "        # and vector b for subsequent layers\n",
    "        params_values['W' + str(layer_idx)] = ? \n",
    "        params_values['b' + str(layer_idx)] = ? \n",
    "        \n",
    "    return params_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P2.b: Implement the forward and backward functions for activation functions (5 Points)\n",
    "1. Sigmoid function\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "2. ReLU function\n",
    "$$\n",
    "relu(x) = \\max\\{0, x\\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid function: sigmoid(X) = 1/(1 + exp(-X))\n",
    "def sigmoid(Z):\n",
    "    sig = ?\n",
    "    return sig\n",
    "\n",
    "def sigmoid_backward(dA, Z):\n",
    "    sig = sigmoid(Z)\n",
    "    dZ = ?\n",
    "    return dZ\n",
    "\n",
    "# ReLU function: relu(Z) = max(0, Z)\n",
    "def relu(Z):\n",
    "    rel = ?\n",
    "    return rel\n",
    "\n",
    "def relu_backward(dA, Z):\n",
    "    dZ = ?\n",
    "    return dZ;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P2.c: Implement the forward pass over MLP (15 Points)\n",
    "We now implement the forward pass over the entire multi-layer perceptron to compute the activations for all the units of MLP. It consists of two functions: \n",
    "1. `single_layer_forward_propagation`: forward pass over a single layer, which is composed of an FC layer followed by an activation function (either ReLU or Sigmoid)\n",
    "2. `full_forward_propagation`: forward pass over the entire MLP network which consists of looping over the layers and calling the `single_layer_forward_propagation`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_layer_forward_propagation(A_prev, W_curr, b_curr, activation=\"relu\"):\n",
    "    \"\"\"\n",
    "    Perform forward propagation over a single layer composed of an FC layer followed by an activation layer. \n",
    "    \n",
    "    Input:\n",
    "        A_prev: np.ndarray(inpdim, nbatch_size), input activations to the current layer\n",
    "        W_curr: np.ndarray(outdim, inpdim), weights of the FC component of the current layer \n",
    "        b_curr: np.ndarray(outdim, 1), biases of the FC component of the current layer \n",
    "        activations: name of the activation layer (either \"relu\" or \"sigmoid\")\n",
    "    \n",
    "    Output:\n",
    "        A_curr: final output of the activation function of the current layer\n",
    "        Z_curr: intermediate input to the activation function\n",
    "    \"\"\"\n",
    "    # calculation of the input value for the activation function\n",
    "    Z_curr = ? \n",
    "    \n",
    "    # selection of activation function\n",
    "    if activation == \"relu\":\n",
    "        activation_func = relu\n",
    "    elif activation == \"sigmoid\":\n",
    "        activation_func = sigmoid\n",
    "    else:\n",
    "        raise Exception('Non-supported activation function')\n",
    "    \n",
    "    # calculate the current activations \n",
    "    A_curr = ? \n",
    "    \n",
    "    # return of calculated activation A and the intermediate Z matrix\n",
    "    return A_curr, Z_curr\n",
    "\n",
    "\n",
    "def full_forward_propagation(X, params_values, nn_architecture):\n",
    "    \"\"\"\n",
    "    Perform forward propagation over full MLP network composed of a single of single layers stacked on top of each other\n",
    "    \n",
    "    Input:\n",
    "        X: np.ndarray(inpdim, nbatch_size), input features to the MLP activations to the current layer\n",
    "        param_values: an array of parameters (weights and biases) returned by the init_layers function\n",
    "        nn_architecture: dictionary of architecture layers\n",
    "    \n",
    "    Output:\n",
    "        A_curr: final prediction of the network\n",
    "        memory: dictionary of intermediate input to the activation function\n",
    "    \"\"\"\n",
    "    # creating a temporary memory to store the information needed for a backward step\n",
    "    memory = {}\n",
    "    # X vector is the activation for layer 0â€Š\n",
    "    A_curr = X\n",
    "    \n",
    "    # iteration over network layers\n",
    "    for idx, layer in enumerate(nn_architecture):\n",
    "        # we number network layers from 1\n",
    "        layer_idx = idx + 1\n",
    "        # transfer the activation from the previous iteration\n",
    "        A_prev = ?\n",
    "        \n",
    "        # extraction of the activation function for the current layer\n",
    "        activ_function_curr = ? \n",
    "        # extraction of W for the current layer\n",
    "        W_curr = ?\n",
    "        # extraction of b for the current layer\n",
    "        b_curr = ?\n",
    "        # calculation of activation for the current layer\n",
    "        A_curr, Z_curr = ?\n",
    "        \n",
    "        # saving calculated values in the memory\n",
    "        memory[\"A\" + str(idx)] = A_prev\n",
    "        memory[\"Z\" + str(layer_idx)] = Z_curr\n",
    "       \n",
    "    # return of prediction vector and a dictionary containing intermediate values\n",
    "    return A_curr, memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions for computing the cost function (nothing to do here)\n",
    "The cross entropy loss\n",
    "$$ L = -\\frac{1}{m} \\left(Y \\log{\\hat{Y}}^T + (1-Y)\\log{(1 - \\hat{Y})}^T \\right) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to compute the cross entropy cost \n",
    "def get_cost_value(Y_hat, Y):\n",
    "    # number of examples\n",
    "    m = Y_hat.shape[1]\n",
    "    # calculation of the cost according to the formula\n",
    "    cost = -1 / m * (np.dot(Y, np.log(Y_hat).T) + np.dot(1 - Y, np.log(1 - Y_hat).T))\n",
    "    return np.squeeze(cost)\n",
    "\n",
    "# an auxiliary function that converts probability into class\n",
    "def convert_prob_into_class(probs):\n",
    "    probs_ = np.copy(probs)\n",
    "    probs_[probs_ > 0.5] = 1\n",
    "    probs_[probs_ <= 0.5] = 0\n",
    "    return probs_\n",
    "\n",
    "# function to get the accuracy of the predictions\n",
    "def get_accuracy_value(Y_hat, Y):\n",
    "    Y_hat_ = convert_prob_into_class(Y_hat)\n",
    "    return (Y_hat_ == Y).all(axis=0).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P2.d: Implement the backward pass over the MLP (15 Points)\n",
    "We now implement the backward pass over the entire multi-layer perceptron to compute the gradients with respect to the activations and the weights in the MLP. It consists of two functions: \n",
    "1. `single_layer_backward_propagation`: backward pass over a single layer, which is composed of an FC layer followed by an activation function (either ReLU or Sigmoid)\n",
    "2. `full_backward_propagation`: backward pass over the entire MLP network which consists of looping over the layers in the reverse order (starting from top) and calling the `single_layer_backward_propagation`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_layer_backward_propagation(dA_curr, W_curr, b_curr, Z_curr, A_prev, activation=\"relu\"):\n",
    "    \"\"\"\n",
    "    Perform forward propagation over a single layer composed of an FC layer followed by an activation layer. \n",
    "    \n",
    "    Input:\n",
    "        dA_curr: gradients from the output activations of the current layer \n",
    "        W_curr: weights of the FC component of the current layer \n",
    "        b_curr: biases of the FC component of the current layer \n",
    "        Z_curr: inputs to the activation function of the current layer\n",
    "        A_prev: inputs to the current layer \n",
    "        activations: name of the activation layer (either \"relu\" or \"sigmoid\")\n",
    "    \n",
    "    Output:\n",
    "        dA_prev: gradients with respect to the inputs of the current layer\n",
    "        dW_curr: gradients with respect to the weights of the FC component of the current layer\n",
    "        db_curr: gradients with respect to the biases of the FC component of the current layer\n",
    "    \"\"\"\n",
    "    # number of examples\n",
    "    m = A_prev.shape[1]\n",
    "    \n",
    "    # selection of activation function\n",
    "    if activation == \"relu\":\n",
    "        backward_activation_func = relu_backward\n",
    "    elif activation == \"sigmoid\":\n",
    "        backward_activation_func = sigmoid_backward\n",
    "    else:\n",
    "        raise Exception('Non-supported activation function')\n",
    "    \n",
    "    # calculation of the activation function derivative\n",
    "    dZ_curr = ? \n",
    "    \n",
    "    # derivative of the matrix W\n",
    "    dW_curr = ? \n",
    "    # derivative of the vector b\n",
    "    db_curr = ?\n",
    "    # derivative of the matrix A_prev\n",
    "    dA_prev = ?\n",
    "\n",
    "    return dA_prev, dW_curr, db_curr\n",
    "\n",
    "def full_backward_propagation(Y_hat, Y, memory, params_values, nn_architecture):\n",
    "    \"\"\"\n",
    "    Perform forward propagation over a single layer composed of an FC layer followed by an activation layer. \n",
    "    \n",
    "    Input:\n",
    "        Y_hat: the output predictions of the MLP\n",
    "        Y: the ground truth value of output\n",
    "        memory: dictionary of activations for units of all layers (computed during the full_forward_propagation)\n",
    "        params_values: dictionary of parameters (weights and biases) of all layers\n",
    "        nn_architecture: dictionary of network layers\n",
    "    \n",
    "    Output:\n",
    "        grad_values: dictionary of gradients of parameters (weights and biases) of all layers\n",
    "    \"\"\"\n",
    "    grads_values = {}\n",
    "    \n",
    "    # number of examples\n",
    "    m = Y.shape[1]\n",
    "    # a hack ensuring the same shape of the prediction vector and labels vector\n",
    "    Y = Y.reshape(Y_hat.shape)\n",
    "    \n",
    "    # initiation of gradient descent algorithm\n",
    "    dA_prev = - (np.divide(Y, Y_hat) - np.divide(1 - Y, 1 - Y_hat));\n",
    "    \n",
    "    for layer_idx_prev, layer in reversed(list(enumerate(nn_architecture))):\n",
    "        # we number network layers from 1\n",
    "        layer_idx_curr = layer_idx_prev + 1\n",
    "        # extraction of the activation function for the current layer\n",
    "        activ_function_curr = ?\n",
    "        \n",
    "        dA_curr = ? \n",
    "        \n",
    "        #get the activations from memory\n",
    "        A_prev = ?\n",
    "        Z_curr = ? \n",
    "        \n",
    "        # get the values of weights and biases from current layer\n",
    "        W_curr = ? \n",
    "        b_curr = ? \n",
    "        \n",
    "        # get the gradients with respect to the inputs, weights, and biases\n",
    "        dA_prev, dW_curr, db_curr = ? \n",
    "        \n",
    "        grads_values[\"dW\" + str(layer_idx_curr)] = dW_curr\n",
    "        grads_values[\"db\" + str(layer_idx_curr)] = db_curr\n",
    "    \n",
    "    return grads_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P2.e: Implement the gradient update function (10 Points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(params_values, grads_values, nn_architecture, learning_rate):\n",
    "    \"\"\"\n",
    "    Perform the parameter update using the gradient descent algorithm\n",
    "    \n",
    "    Input:\n",
    "        params_values: dictionary of parameters (weights and biases for all the layers)\n",
    "        grads_values: dictionary of corresponding gradients of parameters for all layers\n",
    "        nn_architecture: the dictionry of the architecture layers \n",
    "        learning_rate: the scalar learning rate\n",
    "    \n",
    "    Output:\n",
    "        params_values: dictinoary of updated parameters (weights and biases for all the layers)\n",
    "    \"\"\"\n",
    "    # iterate over network layers and update the weights and biases\n",
    "\n",
    "    return params_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top level function for training and plotting the results (nothing to do here)\n",
    "The following cell implements two top level functions for your convenience: \n",
    "\n",
    "1. `train`: top level function called to train the MLP using the dataset \n",
    "2. `plot_metric`: plot the metric as a function of training iterations \n",
    "\n",
    "Please go through the structure of each of these function carefully. In particular pay special attention to the calls to the `full_forward_propagation` and `full_backward_propagation` function and also how and what metrics are being stored for future analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop to train the MLP algorithm. Please carefully read the structure of this function\n",
    "def train(X, Y, nn_architecture, epochs, learning_rate, batch_size=128, verbose=False, callback=None, spam_ids=None):\n",
    "    # initiation of neural net parameters\n",
    "    params_values = init_layers(nn_architecture, 2)\n",
    "    # initiation of lists storing the history \n",
    "    # of metrics calculated during the learning process \n",
    "    cost_history = []\n",
    "    accuracy_history = []\n",
    "    num_samples = X.shape[1]\n",
    "    num_minibatches = int(X.shape[1] / batch_size)\n",
    "    \n",
    "    # performing calculations for subsequent iterations\n",
    "    for i in range(epochs):\n",
    "        for j in range(num_minibatches):\n",
    "            # step forward\n",
    "            inds = np.random.choice(X.shape[1], batch_size // 2)\n",
    "            if spam_ids is not None:\n",
    "                # oversampling minor class\n",
    "                inds_spam = np.random.choice(spam_ids.shape[0], batch_size // 2)\n",
    "                inds_spam = spam_ids[inds_spam]\n",
    "                inds = np.concatenate((inds, inds_spam))\n",
    "            Y_hat, cashe = full_forward_propagation(X[:,inds], params_values, nn_architecture)\n",
    "\n",
    "            # step backward - calculating gradient\n",
    "            grads_values = full_backward_propagation(Y_hat, Y[:, inds], cashe, params_values, nn_architecture)\n",
    "            # updating model state\n",
    "            params_values = update(params_values, grads_values, nn_architecture, learning_rate)\n",
    "\n",
    "        if(i % 10 == 0):\n",
    "            # calculating metrics and saving them in history\n",
    "            cost = get_cost_value(Y_hat, Y[:,inds])\n",
    "            accuracy = get_accuracy_value(Y_hat, Y[:, inds])\n",
    "            cost_history.append(cost)\n",
    "            accuracy_history.append(accuracy)\n",
    "\n",
    "            if(verbose):\n",
    "                print(\"Iteration: {:05} - cost: {:.5f} - accuracy: {:.5f}\".format(i, cost, accuracy))\n",
    "            if(callback is not None):\n",
    "                callback(i, params_values)\n",
    "            \n",
    "    return params_values, cost_history, accuracy_history\n",
    "\n",
    "\n",
    "# a simple function to plot the accuracy of the model as a function of the training iterations\n",
    "def plot_metric(metric, name):\n",
    "    x_axis = np.arange(len(metric))\n",
    "    plt.figure(figsize=(16,12))\n",
    "    axes = plt.gca()\n",
    "    axes.set(xlabel=\"$Step$\", ylabel=name)\n",
    "    plt.title(\"Learning curves\", fontsize=30)\n",
    "    plt.plot(x_axis, np.array(ah))\n",
    "    plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top level function call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_values, ch, ah = train(np.transpose(X_train), np.transpose(y_train.reshape((y_train.shape[0], 1))), NN_ARCHITECTURE, 1000, 0.01, batch_size=64, verbose=True)\n",
    "\n",
    "plot_metric(ah, 'Accuracy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "331px",
    "left": "710px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
